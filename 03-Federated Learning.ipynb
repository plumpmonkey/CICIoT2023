{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the available types of federated learning.\n",
    "\n",
    " - 'STRATIFIED': Stratified sampling of the data. The data is split into a number of shards, and each shard is assigned to a client. The data is split in a stratified manner, meaning that the distribution of the labels is approximately the same in each shard.\n",
    " - 'LEAVE_ONE_OUT' - Each client is assigned a shard of data, each shard is missing one of the attack labels. Other clients in the network are exposed to the attack label, but the specific client is not. This demonstrates the ability of federated learning to protect against unknown attacks.\n",
    " - 'ONE_CLASS' - Each client is assigned a shard of data, each shard contains only one of the attack labels.\n",
    " - 'HALF_BENIGN' - Half of the clients are exposed to Benign data only, the other half are exposed to all data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS SECTION NEEDS TO BE SET TO DETERMINE WHICH CONFIGURATION METHOD TO UTILISE\n",
    "\n",
    "SPLIT_AVAILABLE_METHODS = ['STRATIFIED','LEAVE_ONE_OUT', 'ONE_CLASS', 'HALF_BENIGN' ]\n",
    "METHOD = 'STRATIFIED'\n",
    "NUM_OF_STRATIFIED_CLIENTS = 10  # only applies to stratified method\n",
    "NUM_OF_ROUNDS = 10              # Number of FL rounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above test method in conjunction with the below classification selection will determine the number of clients.\n",
    "\n",
    "EG: \n",
    "`STRATIFIED` with:\n",
    " - `ALL TYPES` - Results in `NUM_OF_STRATIFIED_CLIENTS` clients. Each client will have a stratified sample of the data.\n",
    "\n",
    "`LEAVE_ONE_OUT` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 32 attack labels.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 6 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n",
    "\n",
    "`ONE_CLASS` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 1 attack label.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 1 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n",
    "\n",
    "`HALF_BENIGN` with:\n",
    " - `individual_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 33 malicious attack labels.\n",
    " - `group_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 7 malicious attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_classifier = True\n",
    "group_classifier = False\n",
    "binary_classifier = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the defines for the dataframe columns and the attack labels and their mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from includes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install flwr[simulation] torch torchvision matplotlib sklearn openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from flwr.common import Metrics\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flwr 1.4.0\n",
      "numpy 1.24.2\n",
      "torch 1.13.1\n",
      "Training on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRECTORY = '../datasets/CICIoT2023/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either read the training pickle file if it exists, or process the dataset from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists, loading data...\n",
      "Training data loaded from pickle file.\n",
      "Training data size: (1817320, 47)\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the file 'training_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
    "if os.path.isfile('training_data.pkl'):\n",
    "    print(\"File exists, loading data...\")\n",
    "    train_df = pd.read_pickle('training_data.pkl')\n",
    "    print(\"Training data loaded from pickle file.\")\n",
    "\n",
    "else:\n",
    "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "    df_sets.sort()\n",
    "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
    "\n",
    "    # Print the number of files in each set\n",
    "    print('Training sets: {}'.format(len(training_sets)))\n",
    "    print('Test sets: {}'.format(len(test_sets)))\n",
    "\n",
    "    # ######################\n",
    "    # # TEMP CODE - This would replicate the original authors code with the last CSV \n",
    "    # # for training data. Uncomment this section to use this code.\n",
    "    # ######################\n",
    "    # # Set training_sets to the last entry of training_sets\n",
    "    # training_sets = training_sets[-33:]\n",
    "    # print(f\"TO REPLICATE ORIGINAL AUTHORS CODE WITH ONE FILE TRAIN - {training_sets}\")\n",
    "    # #####################\n",
    "    # # END TEMP CODE\n",
    "    # ######################\n",
    "\n",
    "    # Concatenate all training sets into one dataframe\n",
    "    dfs = []\n",
    "    print(\"Reading training data...\")\n",
    "    for train_set in tqdm(training_sets):\n",
    "        df_new = pd.read_csv(DATASET_DIRECTORY + train_set)\n",
    "        dfs.append(df_new)\n",
    "    train_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Complete training data set size\n",
    "    print(\"Complete training data size: {}\".format(train_df.shape))\n",
    "\n",
    "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
    "    train_df['label'] = train_df['label'].map(dict_34_classes)\n",
    "\n",
    "    # The training data is the 80% of the CSV files in the dataset. The test data is the remaining 20%.\n",
    "    # The Ray Federated learning mechanism cannot cope with all of the 80% training data, so we will split\n",
    "    # the training data using test_train_split. The test data will be ignored as we will use all the data \n",
    "    # from the train_sets files as our training data to keep parity with the original authors code.\n",
    "    # \n",
    "    # By using a subset of the training data split this way, we can have a randomised selection of data\n",
    "    # from all the training CSV files, stratified by the attack types.\n",
    "    \n",
    "    # Percentage of original training data to use.\n",
    "    TRAIN_SIZE = 0.05\n",
    "    \n",
    "    print(f\"Splitting the data into {TRAIN_SIZE*100}%\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_df[X_columns], train_df[y_column], test_size= (1 - TRAIN_SIZE), random_state=42, stratify=train_df[y_column])\n",
    "\n",
    "    # Recombine X_train, and y_train into a dataframe\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # Clean up unused variables\n",
    "\n",
    "    del X_train, y_train, X_test, y_test\n",
    "    \n",
    "    # Save the output to a pickle file\n",
    "    print(\"Writing training data to pickle file...\")\n",
    "    train_df.to_pickle('training_data.pkl')\n",
    "\n",
    "print(\"Training data size: {}\".format(train_df.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of attacks in train_df:\n",
      "6     280286\n",
      "4     210793\n",
      "5     175073\n",
      "2     159342\n",
      "3     157918\n",
      "1     157458\n",
      "7     140127\n",
      "13    129201\n",
      "15    103974\n",
      "14     78948\n",
      "0      42744\n",
      "17     38588\n",
      "19     34660\n",
      "18     29274\n",
      "10     17588\n",
      "26     11980\n",
      "9      11191\n",
      "8      11119\n",
      "25      6955\n",
      "24      5231\n",
      "21      3828\n",
      "22      3193\n",
      "16      2805\n",
      "23      1458\n",
      "12      1117\n",
      "11       907\n",
      "33       504\n",
      "27       231\n",
      "32       213\n",
      "31       204\n",
      "29       148\n",
      "28       125\n",
      "20        87\n",
      "30        50\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# show the unique values counts in the label column for train_df\n",
    "print(\"Counts of attacks in train_df:\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>Header_Length</th>\n",
       "      <th>Protocol Type</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Srate</th>\n",
       "      <th>Drate</th>\n",
       "      <th>fin_flag_number</th>\n",
       "      <th>syn_flag_number</th>\n",
       "      <th>rst_flag_number</th>\n",
       "      <th>...</th>\n",
       "      <th>Std</th>\n",
       "      <th>Tot size</th>\n",
       "      <th>IAT</th>\n",
       "      <th>Number</th>\n",
       "      <th>Magnitue</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Covariance</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Weight</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15361719</th>\n",
       "      <td>0.049451</td>\n",
       "      <td>18424.23</td>\n",
       "      <td>16.89</td>\n",
       "      <td>63.89</td>\n",
       "      <td>8773.276534</td>\n",
       "      <td>8773.276534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.748937</td>\n",
       "      <td>56.23</td>\n",
       "      <td>8.310203e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.057355</td>\n",
       "      <td>3.934737</td>\n",
       "      <td>387.053850</td>\n",
       "      <td>0.02</td>\n",
       "      <td>141.55</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13960994</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>3.162258</td>\n",
       "      <td>3.162258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>8.348236e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.165151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30358725</th>\n",
       "      <td>0.047289</td>\n",
       "      <td>108.46</td>\n",
       "      <td>6.11</td>\n",
       "      <td>69.00</td>\n",
       "      <td>1.180934</td>\n",
       "      <td>1.180934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.804392</td>\n",
       "      <td>58.96</td>\n",
       "      <td>8.294673e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.969024</td>\n",
       "      <td>19.544105</td>\n",
       "      <td>1184.679387</td>\n",
       "      <td>0.18</td>\n",
       "      <td>141.55</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374573</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>63.36</td>\n",
       "      <td>36.746530</td>\n",
       "      <td>36.746530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501296</td>\n",
       "      <td>42.18</td>\n",
       "      <td>8.314974e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.180184</td>\n",
       "      <td>0.710302</td>\n",
       "      <td>2.315749</td>\n",
       "      <td>0.11</td>\n",
       "      <td>141.55</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33149322</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>4.458184</td>\n",
       "      <td>4.458184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>8.315026e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.165151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26352975</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.46</td>\n",
       "      <td>5.94</td>\n",
       "      <td>63.36</td>\n",
       "      <td>24.662729</td>\n",
       "      <td>24.662729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327419</td>\n",
       "      <td>54.06</td>\n",
       "      <td>8.333105e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.404376</td>\n",
       "      <td>0.463495</td>\n",
       "      <td>0.631418</td>\n",
       "      <td>0.18</td>\n",
       "      <td>141.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21622577</th>\n",
       "      <td>4.942958</td>\n",
       "      <td>3631547.56</td>\n",
       "      <td>17.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>1338.337713</td>\n",
       "      <td>1338.337713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>554.00</td>\n",
       "      <td>8.370723e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>33.286634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31273160</th>\n",
       "      <td>0.186219</td>\n",
       "      <td>40025.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>4298.059589</td>\n",
       "      <td>4298.059589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00</td>\n",
       "      <td>8.301589e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27015198</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.39</td>\n",
       "      <td>6.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>18.222303</td>\n",
       "      <td>18.222303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.944377</td>\n",
       "      <td>54.39</td>\n",
       "      <td>8.306414e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.457111</td>\n",
       "      <td>2.752717</td>\n",
       "      <td>23.297396</td>\n",
       "      <td>0.17</td>\n",
       "      <td>141.55</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17580414</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>8.315071e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.165151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1817320 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          flow_duration  Header_Length  Protocol Type  Duration         Rate  \\\n",
       "15361719       0.049451       18424.23          16.89     63.89  8773.276534   \n",
       "13960994       0.000000           0.00           1.00     64.00     3.162258   \n",
       "30358725       0.047289         108.46           6.11     69.00     1.180934   \n",
       "3374573        0.000000           0.00           0.99     63.36    36.746530   \n",
       "33149322       0.000000           0.00           1.00     64.00     4.458184   \n",
       "...                 ...            ...            ...       ...          ...   \n",
       "26352975       0.000000          53.46           5.94     63.36    24.662729   \n",
       "21622577       4.942958     3631547.56          17.00     64.00  1338.337713   \n",
       "31273160       0.186219       40025.00          17.00     64.00  4298.059589   \n",
       "27015198       0.000000          54.39           6.00     64.00    18.222303   \n",
       "17580414       0.000000           0.00           1.00     64.00     0.000000   \n",
       "\n",
       "                Srate  Drate  fin_flag_number  syn_flag_number  \\\n",
       "15361719  8773.276534    0.0              0.0              0.0   \n",
       "13960994     3.162258    0.0              0.0              0.0   \n",
       "30358725     1.180934    0.0              0.0              0.0   \n",
       "3374573     36.746530    0.0              0.0              0.0   \n",
       "33149322     4.458184    0.0              0.0              0.0   \n",
       "...               ...    ...              ...              ...   \n",
       "26352975    24.662729    0.0              0.0              0.0   \n",
       "21622577  1338.337713    0.0              0.0              0.0   \n",
       "31273160  4298.059589    0.0              0.0              0.0   \n",
       "27015198    18.222303    0.0              0.0              0.0   \n",
       "17580414     0.000000    0.0              0.0              0.0   \n",
       "\n",
       "          rst_flag_number  ...        Std  Tot size           IAT  Number  \\\n",
       "15361719              0.0  ...   2.748937     56.23  8.310203e+07     9.5   \n",
       "13960994              0.0  ...   0.000000     42.00  8.348236e+07     9.5   \n",
       "30358725              0.0  ...  13.804392     58.96  8.294673e+07     9.5   \n",
       "3374573               0.0  ...   0.501296     42.18  8.314974e+07     9.5   \n",
       "33149322              0.0  ...   0.000000     42.00  8.315026e+07     9.5   \n",
       "...                   ...  ...        ...       ...           ...     ...   \n",
       "26352975              0.0  ...   0.327419     54.06  8.333105e+07     9.5   \n",
       "21622577              0.0  ...   0.000000    554.00  8.370723e+07     9.5   \n",
       "31273160              0.0  ...   0.000000     50.00  8.301589e+07     9.5   \n",
       "27015198              0.0  ...   1.944377     54.39  8.306414e+07     9.5   \n",
       "17580414              0.0  ...   0.000000     42.00  8.315071e+07     9.5   \n",
       "\n",
       "           Magnitue     Radius   Covariance  Variance  Weight  label  \n",
       "15361719  10.057355   3.934737   387.053850      0.02  141.55      4  \n",
       "13960994   9.165151   0.000000     0.000000      0.00  141.55      6  \n",
       "30358725  10.969024  19.544105  1184.679387      0.18  141.55     15  \n",
       "3374573    9.180184   0.710302     2.315749      0.11  141.55      6  \n",
       "33149322   9.165151   0.000000     0.000000      0.00  141.55      6  \n",
       "...             ...        ...          ...       ...     ...    ...  \n",
       "26352975  10.404376   0.463495     0.631418      0.18  141.55      2  \n",
       "21622577  33.286634   0.000000     0.000000      0.00  141.55     19  \n",
       "31273160  10.000000   0.000000     0.000000      0.00  141.55     13  \n",
       "27015198  10.457111   2.752717    23.297396      0.17  141.55      5  \n",
       "17580414   9.165151   0.000000     0.000000      0.00  141.55      6  \n",
       "\n",
       "[1817320 rows x 47 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Data\n",
    "Concat the test data into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File testing_data.pkl exists, loading data...\n",
      "Test data loaded from pickle file.\n",
      "Testing data size: (10340161, 47)\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the file 'test_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
    "testing_data_pickle_file = 'testing_data.pkl'\n",
    "\n",
    "if os.path.isfile(testing_data_pickle_file):\n",
    "    print(f\"File {testing_data_pickle_file} exists, loading data...\")\n",
    "    test_df = pd.read_pickle(testing_data_pickle_file)\n",
    "    print(\"Test data loaded from pickle file.\")\n",
    "\n",
    "else:\n",
    "    print(f\"File {testing_data_pickle_file} does not exist, constructing data...\")\n",
    "\n",
    "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "    df_sets.sort()\n",
    "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
    "\n",
    "    # Print the number of files in each set\n",
    "    print('Test sets: {}'.format(len(test_sets)))\n",
    "    \n",
    "    # Concatenate all testing sets into one dataframe\n",
    "    dfs = []\n",
    "    print(\"Reading test data...\")\n",
    "    for test_set in tqdm(test_sets):\n",
    "        df_new = pd.read_csv(DATASET_DIRECTORY + test_set)\n",
    "        dfs.append(df_new)\n",
    "    test_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
    "    test_df['label'] = test_df['label'].map(dict_34_classes)\n",
    "\n",
    "    # Save the output to a pickle file\n",
    "    print(f\"Writing test data to pickle file {testing_data_pickle_file}...\")\n",
    "    test_df.to_pickle(testing_data_pickle_file)\n",
    "\n",
    "print(\"Testing data size: {}\".format(test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train_df: 1817320\n",
      "Number of rows in test_df: 10340161\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in train_df: {}\".format(len(train_df)))\n",
    "print(\"Number of rows in test_df: {}\".format(len(test_df)))\n",
    "\n",
    "train_size = len(train_df)\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Scale the test and train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the training data input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df[X_columns] = scaler.fit_transform(train_df[X_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the testing data input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[X_columns] = scaler.fit_transform(test_df[X_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define the classification problem - (2 classes, 8 classes or 34 classes)\n",
    "Change the following cell to select the classification type\n",
    "\n",
    "If the METHOD == STRATIFIED, then we can use any classifier\n",
    "If the METHOD == ATTACK_GROUP then we must use Group Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary 2 Class Classifier... - Adjusting labels in test and train dataframes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_size_map = {2: \"Binary\", 8: \"Group\", 34: \"Individual\"}\n",
    "\n",
    "if group_classifier:\n",
    "    print(\"Group 8 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
    "    # Map y column to the dict_7_classes values\n",
    "    test_df['label'] = test_df['label'].map(dict_8_classes)\n",
    "    train_df['label'] = train_df['label'].map(dict_8_classes)\n",
    "    class_size = \"8\"      \n",
    "    \n",
    "elif binary_classifier:\n",
    "    print(\"Binary 2 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
    "    # Map y column to the dict_2_classes values\n",
    "    test_df['label'] = test_df['label'].map(dict_2_classes)\n",
    "    train_df['label'] = train_df['label'].map(dict_2_classes)\n",
    "    class_size = \"2\"\n",
    "\n",
    "else:\n",
    "    print (\"Individual 34 Class classifier... - No adjustments to labels in test and train dataframes\")\n",
    "    class_size = \"34\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Split the Training Data into partitions for the Federated Learning clients depending on the test required\n",
    "As a reminder:\n",
    "\n",
    "`STRATIFIED` with:\n",
    " - `ALL TYPES` - Results in `NUM_OF_STRATIFIED_CLIENTS` clients. Each client will have a stratified sample of the data.\n",
    "\n",
    "`LEAVE_ONE_OUT` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 32 attack labels.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 6 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n",
    "\n",
    "`ONE_CLASS` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 1 attack label.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 1 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n",
    "\n",
    "`HALF_BENIGN` with:\n",
    " - `individual_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 33 malicious attack labels.\n",
    " - `group_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 7 malicious attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mHALF_BENIGN METHOD\u001b[0m with 2 class classifier\n",
      "Benign only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26232\\697725314.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes\n",
      "Benign only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26232\\697725314.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes\n",
      "Benign only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26232\\697725314.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes\n",
      "Benign only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26232\\697725314.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes\n",
      "Benign only\n",
      "All Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26232\\697725314.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define fl_X_train and fl_y_train\n",
    "fl_X_train = []\n",
    "fl_y_train = []\n",
    "\n",
    "client_df = pd.DataFrame()\n",
    "\n",
    "if METHOD == 'STRATIFIED':\n",
    "    print(f\"{Colours.YELLOW.value}STRATIFIED METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "    # We are going to split the training data into 'NUM_OF_STRATIFIED_CLIENTS' smaller groups using StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=NUM_OF_STRATIFIED_CLIENTS, shuffle=True, random_state=42)\n",
    "    for train_index, test_index in skf.split(train_df[X_columns], train_df[y_column]):\n",
    "        fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "        fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "\n",
    "elif METHOD == 'LEAVE_ONE_OUT':\n",
    "    print(f\"{Colours.YELLOW.value}LEAVE_ONE_OUT METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "\n",
    "    if individual_classifier or group_classifier:\n",
    "        # Set the number of splits required to the number of classes - 1\n",
    "        num_splits = int(class_size) - 1\n",
    "    else:\n",
    "        # For binary classifier, set the number of splits to 10\n",
    "        num_splits = 10\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # When creating the clients, we will remove one attack class from the training data\n",
    "    # For the binary classifier, evey other client will have the benign class removed\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
    "        if binary_classifier:\n",
    "            print(f\"i: {i} = i % 2 = {i % 2}\")\n",
    "            if i % 2 == 0:\n",
    "                print(\"Benign only\")\n",
    "                # Create a new dataframe for the client data with only benign traffic\n",
    "                client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != 1]], ignore_index=True)\n",
    "                fl_X_train.append(client_df[X_columns])\n",
    "                fl_y_train.append(client_df[y_column])\n",
    "            else:\n",
    "                print(\"Both\")\n",
    "                # Create a new dataframe for the client data\n",
    "                fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "                fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "        else:\n",
    "            # Create a new dataframe for the client data\n",
    "            client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != i+1]], ignore_index=True)\n",
    "            fl_X_train.append(client_df[X_columns])\n",
    "            fl_y_train.append(client_df[y_column])\n",
    "\n",
    "elif METHOD == 'ONE_CLASS':\n",
    "    print(f\"{Colours.YELLOW.value}ONE_CLASS METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "    # Each client only has one attack class in their training data along with the Benign data\n",
    "    \n",
    "    if individual_classifier or group_classifier:\n",
    "        # Set the number of splits required to the number of classes - 1\n",
    "        num_splits = int(class_size) - 1\n",
    "    else:\n",
    "        # For binary classifier, set the number of splits to 10\n",
    "        num_splits = 10\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # When creating the clients, we will only add the benign data and the attack class for that client\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
    "        if binary_classifier:\n",
    "            print(f\"i: {i} = i % 2 = {i % 2}\")\n",
    "            if i % 2 == 0:\n",
    "                print(\"Benign only\")\n",
    "                # Create a new dataframe for the client data with only benign traffic\n",
    "                client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != 1]], ignore_index=True)\n",
    "                fl_X_train.append(client_df[X_columns])\n",
    "                fl_y_train.append(client_df[y_column])\n",
    "            else:\n",
    "                print(\"Both\")\n",
    "                # Create a new dataframe for the client data\n",
    "                fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "                fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "        else:\n",
    "            # Create a new dataframe for the client data\n",
    "            client_df = pd.concat([train_df.iloc[test_index][(train_df[y_column] == 0) | (train_df[y_column] == i+1)]], ignore_index=True)\n",
    "            fl_X_train.append(client_df[X_columns])\n",
    "            fl_y_train.append(client_df[y_column])\n",
    "\n",
    "elif METHOD == 'HALF_BENIGN':\n",
    "    print(f\"{Colours.YELLOW.value}HALF_BENIGN METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "\n",
    "    num_splits = 10\n",
    "\n",
    "    # Split into 10 client data\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    # For i % 2 == 0, add only benign data\n",
    "    # For i % 2 == 1, add all data\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
    "        if i % 2 == 0:\n",
    "            print(\"Benign only\")\n",
    "            # Create a new dataframe for the client data with only benign traffic\n",
    "            client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n",
    "            fl_X_train.append(client_df[X_columns])\n",
    "            fl_y_train.append(client_df[y_column])\n",
    "        else:\n",
    "            print(\"All Classes\")\n",
    "            fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "            fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "else:\n",
    "    print(f\"{Colours.RED.value}ERROR: Method {METHOD} not recognised{Colours.NORMAL.value}\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fl_X_train[0].shape: (4274, 46)\n",
      "fl_y_train[0].value_counts():\n",
      "0    4274\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[0].unique(): [0]\n",
      "\n",
      "fl_X_train[1].shape: (181732, 46)\n",
      "fl_y_train[1].value_counts():\n",
      "1    177458\n",
      "0      4274\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[1].unique(): [1 0]\n",
      "\n",
      "fl_X_train[2].shape: (4274, 46)\n",
      "fl_y_train[2].value_counts():\n",
      "0    4274\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[2].unique(): [0]\n",
      "\n",
      "fl_X_train[3].shape: (181732, 46)\n",
      "fl_y_train[3].value_counts():\n",
      "1    177458\n",
      "0      4274\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[3].unique(): [1 0]\n",
      "\n",
      "fl_X_train[4].shape: (4274, 46)\n",
      "fl_y_train[4].value_counts():\n",
      "0    4274\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[4].unique(): [0]\n",
      "\n",
      "fl_X_train[5].shape: (181732, 46)\n",
      "fl_y_train[5].value_counts():\n",
      "1    177458\n",
      "0      4274\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[5].unique(): [1 0]\n",
      "\n",
      "fl_X_train[6].shape: (4275, 46)\n",
      "fl_y_train[6].value_counts():\n",
      "0    4275\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[6].unique(): [0]\n",
      "\n",
      "fl_X_train[7].shape: (181732, 46)\n",
      "fl_y_train[7].value_counts():\n",
      "1    177457\n",
      "0      4275\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[7].unique(): [1 0]\n",
      "\n",
      "fl_X_train[8].shape: (4275, 46)\n",
      "fl_y_train[8].value_counts():\n",
      "0    4275\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[8].unique(): [0]\n",
      "\n",
      "fl_X_train[9].shape: (181732, 46)\n",
      "fl_y_train[9].value_counts():\n",
      "1    177457\n",
      "0      4275\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[9].unique(): [1 0]\n",
      "\n",
      "fl_X_train[0].equals(fl_X_train[1]): False\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_CLIENTS = len(fl_X_train)\n",
    "\n",
    "for i in range(len(fl_X_train)):\n",
    "    # Show the unique values in the y column\n",
    "    (f\"Client ID: {i}\")\n",
    "    print(f\"fl_X_train[{i}].shape: {fl_X_train[i].shape}\")  \n",
    "    print(f\"fl_y_train[{i}].value_counts():\\n{fl_y_train[i].value_counts()}\")\n",
    "    print(f\"fl_y_train[{i}].unique(): {fl_y_train[i].unique()}\\n\")\n",
    "\n",
    "# Check that fl_X_train[0] and fl_X_train[1] contain different data\n",
    "print(f\"fl_X_train[0].equals(fl_X_train[1]): {fl_X_train[0].equals(fl_X_train[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare an output directory where we can store the results of the federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an \"Output\" directory if it doesnt exist already\n",
    "if not os.path.exists(\"Output\"):\n",
    "    os.makedirs(\"Output\")\n",
    "\n",
    "sub_dir_name = f\"train_size-{train_size}\"\n",
    "\n",
    "# if sub_dir_name does not exist, create it\n",
    "if not os.path.exists(f\"Output/{sub_dir_name}\"):\n",
    "    os.makedirs(f\"Output/{sub_dir_name}\")\n",
    "\n",
    "test_directory_name = f\"{METHOD}_Classifier-{class_size}_Clients-{NUM_OF_CLIENTS}\"\n",
    "\n",
    "# Create an \"Output/{METHOD}-{NUM_OF_CLIENTS}-{NUM_OF_ROUNDS}\" directory if it doesnt exist already\n",
    "if not os.path.exists(f\"Output/{sub_dir_name}/{test_directory_name}\"):\n",
    "    os.makedirs(f\"Output/{sub_dir_name}/{test_directory_name}\")\n",
    "\n",
    "# Ensure the directory is empty\n",
    "for file in os.listdir(f\"Output/{sub_dir_name}/{test_directory_name}\"):\n",
    "    file_path = os.path.join(f\"Output/{sub_dir_name}/{test_directory_name}\", file)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.unlink(file_path)\n",
    "\n",
    "# Original training size is the sum of all the fl_X_train sizes\n",
    "original_training_size = 0\n",
    "for i in range(len(fl_X_train)):\n",
    "    original_training_size += fl_X_train[i].shape[0]\n",
    "\n",
    "# Write this same info to the output directory/Class Split Info.txt\n",
    "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Class Split Info.txt\", \"w\") as f:\n",
    "    for i in range(len(fl_X_train)):\n",
    "        f.write(f\"Client ID: {i}\\n\")\n",
    "        f.write(f\"fl_X_train.shape: {fl_X_train[i].shape}\\n\")\n",
    "        f.write(f\"Training data used {original_training_size}\")\n",
    "        f.write(f\"fl_y_train.value_counts():\\n{fl_y_train[i].value_counts()}\\n\")\n",
    "        f.write(f\"fl_y_train.unique(): {fl_y_train[i].unique()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the testing daya to X_test and y_test ndarrays\n",
    "X_test = test_df[X_columns].to_numpy()\n",
    "y_test = test_df[y_column].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_classes = len(train_df[y_column].unique())\n",
    "\n",
    "train_df_shape = train_df.shape\n",
    "test_df_shape = test_df.shape\n",
    "\n",
    "# We are now done with the train_df and test_df dataframes, so we can delete them to free up memory\n",
    "del train_df\n",
    "del test_df\n",
    "del client_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CLIENTS: 10\n",
      "NUM_ROUNDS: 10\n",
      "\n",
      "Original training size: 930032\n",
      "Checking training data split groups\n",
      "0 : X Shape (4274, 46) Y Shape (4274,)\n",
      "1 : X Shape (181732, 46) Y Shape (181732,)\n",
      "2 : X Shape (4274, 46) Y Shape (4274,)\n",
      "3 : X Shape (181732, 46) Y Shape (181732,)\n",
      "4 : X Shape (4274, 46) Y Shape (4274,)\n",
      "5 : X Shape (181732, 46) Y Shape (181732,)\n",
      "6 : X Shape (4275, 46) Y Shape (4275,)\n",
      "7 : X Shape (181732, 46) Y Shape (181732,)\n",
      "8 : X Shape (4275, 46) Y Shape (4275,)\n",
      "9 : X Shape (181732, 46) Y Shape (181732,)\n",
      "\n",
      "Checking testing data\n",
      "X_test size: (10340161, 46)\n",
      "y_test size: (10340161,)\n",
      "\n",
      "Deploy Simulation\n"
     ]
    }
   ],
   "source": [
    "print(\"NUM_CLIENTS:\", NUM_OF_CLIENTS)\n",
    "\n",
    "print(\"NUM_ROUNDS:\", NUM_OF_ROUNDS)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Original training size: {}\".format(original_training_size))\n",
    "\n",
    "\n",
    "print(\"Checking training data split groups\")\n",
    "for i in range(len(fl_X_train)):\n",
    "    print(i, \":\", \"X Shape\", fl_X_train[i].shape, \"Y Shape\", fl_y_train[i].shape)\n",
    "\n",
    "\n",
    "# Print the sizes of X_test and y_test\n",
    "print(\"\\nChecking testing data\")\n",
    "print(\"X_test size: {}\".format(X_test.shape))\n",
    "print(\"y_test size: {}\".format(y_test.shape))\n",
    "\n",
    "print(\"\\nDeploy Simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Federated Learning\n",
    "## Import the libraries and print the versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Client and Server code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn 1.2.0.\n",
      "flwr 1.4.0\n",
      "numpy 1.24.2\n",
      "tf 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print('scikit-learn {}.'.format(sklearn.__version__))\n",
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"tf\", tf.__version__)\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "import datetime\n",
    "\n",
    "client_evaluations = []\n",
    "\n",
    "class NumpyFlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, model, train_data, train_labels):\n",
    "        self.model = model\n",
    "        self.cid = cid\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Training...\")\n",
    "        self.model.fit(self.train_data, self.train_labels, epochs=5, batch_size=32)\n",
    "        print (\"Client \", self.cid, \"Training complete...\")\n",
    "        return self.model.get_weights(), len(self.train_data), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Evaluating...\")\n",
    "        loss, accuracy = self.model.evaluate(self.train_data, self.train_labels, batch_size=32)\n",
    "        print(f\"{Colours.YELLOW.value}Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}{Colours.NORMAL.value}\")\n",
    "\n",
    "        # Write the same message to the \"Output/{cid}_Evaluation.txt\" file\n",
    "        with open(f\"Output/{sub_dir_name}/{test_directory_name}/{self.cid}_Evaluation.txt\", \"a\") as f:\n",
    "            f.write(f\"{datetime.datetime.now()} - Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}\\n\")\n",
    "\n",
    "            # Close the file\n",
    "            f.close()\n",
    "\n",
    "        return loss, len(self.train_data), {\"accuracy\": accuracy}\n",
    "    \n",
    "    def predict(self, incoming):\n",
    "        prediction = np.argmax( self.model.predict(incoming) ,axis=1)\n",
    "        return prediction\n",
    "\n",
    "def client_fn(cid: str) -> NumpyFlowerClient:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    #model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n",
    "    #model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    print (\"Client ID:\", cid)\n",
    "\n",
    "    model = Sequential([\n",
    "      #Flatten(input_shape=(79,1)),\n",
    "      Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "      Dense(50, activation='relu'),  \n",
    "      Dense(25, activation='relu'),  \n",
    "      Dense(num_unique_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "   \n",
    "    partition_id = int(cid)\n",
    "    X_train_c = fl_X_train[partition_id]\n",
    "    y_train_c = fl_y_train[partition_id]\n",
    "\n",
    "    # Create a  single Flower client representing a single organization\n",
    "    return NumpyFlowerClient(cid, model, X_train_c, y_train_c)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "eval_count = 0\n",
    "\n",
    "def get_evaluate_fn(server_model):\n",
    "    global eval_count\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "    # The `evaluate` function will be called after every round\n",
    "    \n",
    "    \n",
    "    def evaluate(server_round, parameters, config):\n",
    "        global eval_count\n",
    "        \n",
    "        # Update model with the latest parameters\n",
    "        server_model.set_weights(parameters)\n",
    "        print (f\"Server Evaluating... Evaluation Count:{eval_count}\")\n",
    "        loss, accuracy = server_model.evaluate(X_test, y_test)\n",
    "        \n",
    "        y_pred = server_model.predict(X_test)\n",
    "        print (\"Prediction: \", y_pred, y_pred.shape)\n",
    "        #cmatrix = confusion_matrix(y_test, np.rint(y_pred))\n",
    "        #print (\"confusion_matrix:\", cmatrix, cmatrix.shape)\n",
    "                        \n",
    "        print(f\"{Colours.YELLOW.value}Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}{Colours.NORMAL.value}\")\n",
    "\n",
    "        # Write the same message to the \"Output/Server_Evaluation.txt\" file\n",
    "        with open(f\"Output/{sub_dir_name}/{test_directory_name}/Server_Evaluation.txt\", \"a\") as f:\n",
    "            f.write(f\"{datetime.datetime.now()} - {server_round} : Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\\n\")\n",
    "\n",
    "            # Close the file\n",
    "            f.close()\n",
    "        \n",
    "        np.save(\"y_pred-\" + str(eval_count) + \".npy\", y_pred)\n",
    "        #np.save(\"cmatrix-\" + str(eval_count) + \".npy\", cmatrix)\n",
    "        eval_count = eval_count + 1\n",
    "        \n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "    return evaluate\n",
    "\n",
    "\n",
    "\n",
    "server_model = Sequential([\n",
    "    #Flatten(input_shape=(79,1)),\n",
    "    Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "    Dense(50, activation='relu'),  \n",
    "    Dense(25, activation='relu'),  \n",
    "    Dense(num_unique_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "server_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=2, #10,\n",
    "        min_evaluate_clients=2, #5,\n",
    "        min_available_clients=2, #10,\n",
    "        evaluate_fn=get_evaluate_fn(server_model),\n",
    "        #evaluate_metrics_aggregation_fn=weighted_average,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-07-15 13:41:55,623 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\n",
      "Deploy simulation... Method = HALF_BENIGN - Binary (2) Classifier\n",
      "Number of Clients = 10\n",
      "\n",
      "Writing output to: train_size-1817320/HALF_BENIGN_Classifier-2_Clients-10\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-15 13:42:01,384\tINFO worker.py:1636 -- Started a local Ray instance.\n",
      "INFO flwr 2023-07-15 13:42:05,582 | app.py:180 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'memory': 31215680718.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 15607840358.0, 'CPU': 24.0}\n",
      "INFO flwr 2023-07-15 13:42:05,583 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-07-15 13:42:05,583 | server.py:273 | Requesting initial parameters from one random client\n",
      "INFO flwr 2023-07-15 13:42:09,739 | server.py:277 | Received initial parameters from one random client\n",
      "INFO flwr 2023-07-15 13:42:09,739 | server.py:88 | Evaluating initial parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=31984)\u001b[0m Client ID: 2\n",
      "Server Evaluating... Evaluation Count:0\n",
      "323131/323131 [==============================] - 229s 707us/step - loss: 1.0544 - accuracy: 0.1565\n",
      "323131/323131 [==============================] - 209s 646us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-07-15 13:51:36,362 | server.py:91 | initial parameters (loss, other metrics): 1.0543588399887085, {'accuracy': 0.15654368698596954}\n",
      "INFO flwr 2023-07-15 13:51:36,363 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-07-15 13:51:36,364 | server.py:218 | fit_round 1: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [[0.67037696 0.329623  ]\n",
      " [0.7040978  0.2959022 ]\n",
      " [0.64444405 0.35555592]\n",
      " ...\n",
      " [0.4926274  0.5073726 ]\n",
      " [0.6486422  0.3513578 ]\n",
      " [0.6675789  0.33242112]] (10340161, 2)\n",
      "\u001b[33mServer evaluation complete - Accuracy: 0.1565, Loss: 1.0544\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m Client ID: 0\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m Client  0 Training...\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m Epoch 1/5\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m   1/134 [..............................] - ETA: 1:25 - loss: 0.4063 - accuracy: 0.8125\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m  37/134 [=======>......................] - ETA: 0s - loss: 0.0521 - accuracy: 0.9882  \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m  76/134 [================>.............] - ETA: 0s - loss: 0.0260 - accuracy: 0.9942\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m 114/134 [========================>.....] - ETA: 0s - loss: 0.0175 - accuracy: 0.9962\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m 134/134 [==============================] - 1s 1ms/step - loss: 0.0151 - accuracy: 0.9967\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m Epoch 2/5\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m   1/134 [..............................] - ETA: 0s - loss: 5.6994e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m  43/134 [========>.....................] - ETA: 0s - loss: 4.3513e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m  81/134 [=================>............] - ETA: 0s - loss: 3.6878e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m 123/134 [==========================>...] - ETA: 0s - loss: 3.3941e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m 134/134 [==============================] - 0s 1ms/step - loss: 3.2683e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m Epoch 3/5\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m   1/134 [..............................] - ETA: 0s - loss: 1.2126e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m  40/134 [=======>......................] - ETA: 0s - loss: 1.8953e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m  81/134 [=================>............] - ETA: 0s - loss: 1.6497e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m 125/134 [==========================>...] - ETA: 0s - loss: 1.4486e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m 134/134 [==============================] - 0s 1ms/step - loss: 1.4270e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m Epoch 4/5\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m   1/134 [..............................] - ETA: 0s - loss: 1.1685e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m  46/134 [=========>....................] - ETA: 0s - loss: 8.4437e-05 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m  90/134 [===================>..........] - ETA: 0s - loss: 7.7840e-05 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m 130/134 [============================>.] - ETA: 0s - loss: 7.1943e-05 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m 134/134 [==============================] - 0s 1ms/step - loss: 7.1669e-05 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m Epoch 5/5\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m   1/134 [..............................] - ETA: 0s - loss: 6.3001e-05 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m  45/134 [=========>....................] - ETA: 0s - loss: 4.9174e-05 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m  90/134 [===================>..........] - ETA: 0s - loss: 4.5260e-05 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m 134/134 [==============================] - ETA: 0s - loss: 4.3631e-05 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m 134/134 [==============================] - 0s 1ms/step - loss: 4.3631e-05 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31984)\u001b[0m Client  0 Training complete...\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=32044)\u001b[0m Client ID: 2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=32044)\u001b[0m Client  2 Training...\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31468)\u001b[0m Client  8 Training...\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=32044)\u001b[0m Epoch 1/5\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30712)\u001b[0m   1/134 [..............................] - ETA: 1:30 - loss: 0.6009 - accuracy: 0.6562\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30712)\u001b[0m   1/134 [..............................] - ETA: 1:30 - loss: 0.6009 - accuracy: 0.6562\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=32044)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=32044)\u001b[0m   1/134 [..............................] - ETA: 1:31 - loss: 0.7086 - accuracy: 0.6875\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=32044)\u001b[0m  38/134 [=======>......................] - ETA: 0s - loss: 0.0648 - accuracy: 0.9836  \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=32044)\u001b[0m  76/134 [================>.............] - ETA: 0s - loss: 0.0330 - accuracy: 0.9918\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31608)\u001b[0m  74/134 [===============>..............] - ETA: 0s - loss: 0.0306 - accuracy: 0.9899\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31608)\u001b[0m 112/134 [========================>.....] - ETA: 0s - loss: 0.0205 - accuracy: 0.9933\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30712)\u001b[0m 110/134 [=======================>......] - ETA: 0s - loss: 0.0213 - accuracy: 0.9937\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=32044)\u001b[0m 134/134 [==============================] - 1s 1ms/step - loss: 0.0191 - accuracy: 0.9953\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=32044)\u001b[0m 117/134 [=========================>....] - ETA: 0s - loss: 4.0399e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  227/5680 [>.............................] - ETA: 7s - loss: 0.1651 - accuracy: 0.9419\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  268/5680 [>.............................] - ETA: 7s - loss: 0.1453 - accuracy: 0.9486\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30712)\u001b[0m  41/134 [========>.....................] - ETA: 0s - loss: 1.8182e-04 - accuracy: 1.0000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30832)\u001b[0m  394/5680 [=>............................] - ETA: 6s - loss: 0.1126 - accuracy: 0.9610\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30832)\u001b[0m  590/5680 [==>...........................] - ETA: 6s - loss: 0.0846 - accuracy: 0.9695\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=32044)\u001b[0m Client  2 Training complete...\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  786/5680 [===>..........................] - ETA: 6s - loss: 0.0719 - accuracy: 0.9738\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  978/5680 [====>.........................] - ETA: 5s - loss: 0.0626 - accuracy: 0.9767\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30832)\u001b[0m 1136/5680 [=====>........................] - ETA: 5s - loss: 0.0569 - accuracy: 0.9784\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 1355/5680 [======>.......................] - ETA: 5s - loss: 0.0503 - accuracy: 0.9809\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30832)\u001b[0m 1895/5680 [=========>....................] - ETA: 4s - loss: 0.0438 - accuracy: 0.9829\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 2102/5680 [==========>...................] - ETA: 4s - loss: 0.0407 - accuracy: 0.9836\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30832)\u001b[0m 2289/5680 [===========>..................] - ETA: 3s - loss: 0.0403 - accuracy: 0.9841\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2470/5680 [============>.................] - ETA: 3s - loss: 0.0404 - accuracy: 0.9838\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2520/5680 [============>.................] - ETA: 3s - loss: 0.0400 - accuracy: 0.9840\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 2657/5680 [=============>................] - ETA: 3s - loss: 0.0369 - accuracy: 0.9848\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 2707/5680 [=============>................] - ETA: 3s - loss: 0.0366 - accuracy: 0.9849\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2871/5680 [==============>...............] - ETA: 3s - loss: 0.0386 - accuracy: 0.9845\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 3422/5680 [=================>............] - ETA: 2s - loss: 0.0336 - accuracy: 0.9862\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 3472/5680 [=================>............] - ETA: 2s - loss: 0.0355 - accuracy: 0.9862\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 3629/5680 [==================>...........] - ETA: 2s - loss: 0.0348 - accuracy: 0.9864\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 3679/5680 [==================>...........] - ETA: 2s - loss: 0.0346 - accuracy: 0.9865\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3832/5680 [===================>..........] - ETA: 1s - loss: 0.0342 - accuracy: 0.9861\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m Client ID: 5\u001b[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m Client  5 Training...\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 3985/5680 [====================>.........] - ETA: 1s - loss: 0.0339 - accuracy: 0.9867\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31608)\u001b[0m Epoch 5/5\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 4188/5680 [=====================>........] - ETA: 1s - loss: 0.0333 - accuracy: 0.9869\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 4235/5680 [=====================>........] - ETA: 1s - loss: 0.0332 - accuracy: 0.9870\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 4385/5680 [======================>.......] - ETA: 1s - loss: 0.0327 - accuracy: 0.9872\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 4435/5680 [======================>.......] - ETA: 1s - loss: 0.0326 - accuracy: 0.9872\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m \u001b[32m [repeated 432x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31608)\u001b[0m   1/134 [..............................] - ETA: 0s - loss: 7.1540e-05 - accuracy: 1.0000\u001b[32m [repeated 46x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 1667/5680 [=======>......................] - ETA: 4s - loss: 0.0467 - accuracy: 0.9816\u001b[32m [repeated 38x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 3371/5680 [================>.............] - ETA: 2s - loss: 0.0351 - accuracy: 0.9856\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 3217/5680 [===============>..............] - ETA: 2s - loss: 0.0352 - accuracy: 0.9855\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4830/5680 [========================>.....] - ETA: 0s - loss: 0.0315 - accuracy: 0.9869\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4723/5680 [=======================>......] - ETA: 1s - loss: 0.0316 - accuracy: 0.9869\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31608)\u001b[0m 134/134 [==============================] - 0s 1ms/step - loss: 2.4541e-05 - accuracy: 1.0000\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 5114/5680 [==========================>...] - ETA: 0s - loss: 0.0309 - accuracy: 0.9876\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 5166/5680 [==========================>...] - ETA: 0s - loss: 0.0308 - accuracy: 0.9876\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 5043/5680 [=========================>....] - ETA: 0s - loss: 0.0302 - accuracy: 0.9873\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m  377/5680 [>.............................] - ETA: 7s - loss: 0.1101 - accuracy: 0.9600\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 1869/5680 [========>.....................] - ETA: 4s - loss: 0.0445 - accuracy: 0.9825\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m  565/5680 [=>............................] - ETA: 6s - loss: 0.0837 - accuracy: 0.9686\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 5323/5680 [===========================>..] - ETA: 0s - loss: 0.0305 - accuracy: 0.9877\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 5371/5680 [===========================>..] - ETA: 0s - loss: 0.0305 - accuracy: 0.9877\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 5527/5680 [============================>.] - ETA: 0s - loss: 0.0303 - accuracy: 0.9878\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m  741/5680 [==>...........................] - ETA: 6s - loss: 0.0707 - accuracy: 0.9744\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31608)\u001b[0m Client  6 Training complete...\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m  937/5680 [===>..........................] - ETA: 5s - loss: 0.0610 - accuracy: 0.9774\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 1108/5680 [====>.........................] - ETA: 5s - loss: 0.0588 - accuracy: 0.9780\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 1286/5680 [=====>........................] - ETA: 5s - loss: 0.0537 - accuracy: 0.9797\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 1473/5680 [======>.......................] - ETA: 5s - loss: 0.0495 - accuracy: 0.9810\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 2066/5680 [=========>....................] - ETA: 4s - loss: 0.0423 - accuracy: 0.9834\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 2266/5680 [==========>...................] - ETA: 3s - loss: 0.0406 - accuracy: 0.9840\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 2454/5680 [===========>..................] - ETA: 3s - loss: 0.0382 - accuracy: 0.9843\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 2617/5680 [============>.................] - ETA: 3s - loss: 0.0383 - accuracy: 0.9847\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 2817/5680 [=============>................] - ETA: 3s - loss: 0.0372 - accuracy: 0.9848\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 3016/5680 [==============>...............] - ETA: 2s - loss: 0.0362 - accuracy: 0.9852\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 3578/5680 [=================>............] - ETA: 2s - loss: 0.0343 - accuracy: 0.9858\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 3782/5680 [==================>...........] - ETA: 2s - loss: 0.0342 - accuracy: 0.9866\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 3934/5680 [===================>..........] - ETA: 1s - loss: 0.0331 - accuracy: 0.9862\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 4134/5680 [====================>.........] - ETA: 1s - loss: 0.0324 - accuracy: 0.9865\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m Epoch 2/5\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 4329/5680 [=====================>........] - ETA: 1s - loss: 0.0318 - accuracy: 0.9867\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 4535/5680 [======================>.......] - ETA: 1s - loss: 0.0323 - accuracy: 0.9872\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m \u001b[32m [repeated 280x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m  154/5680 [..............................] - ETA: 5s - loss: 0.0190 - accuracy: 0.9905\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1701/5680 [=======>......................] - ETA: 3s - loss: 0.0201 - accuracy: 0.9910\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 3386/5680 [================>.............] - ETA: 2s - loss: 0.0198 - accuracy: 0.9912\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 3189/5680 [===============>..............] - ETA: 2s - loss: 0.0200 - accuracy: 0.9911\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 4888/5680 [========================>.....] - ETA: 0s - loss: 0.0306 - accuracy: 0.9872\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 4725/5680 [=======================>......] - ETA: 1s - loss: 0.0311 - accuracy: 0.9870\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 5680/5680 [==============================] - 7s 1ms/step - loss: 0.0291 - accuracy: 0.9876\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 5296/5680 [==========================>...] - ETA: 0s - loss: 0.0298 - accuracy: 0.9875\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 5093/5680 [=========================>....] - ETA: 0s - loss: 0.0302 - accuracy: 0.9873\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m  354/5680 [>.............................] - ETA: 5s - loss: 0.0224 - accuracy: 0.9900\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1891/5680 [========>.....................] - ETA: 3s - loss: 0.0201 - accuracy: 0.9911\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m  564/5680 [=>............................] - ETA: 5s - loss: 0.0220 - accuracy: 0.9900\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5489/5680 [===========================>..] - ETA: 0s - loss: 0.0304 - accuracy: 0.9874\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 5650/5680 [============================>.] - ETA: 0s - loss: 0.0291 - accuracy: 0.9877\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m  754/5680 [==>...........................] - ETA: 4s - loss: 0.0218 - accuracy: 0.9908\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m  919/5680 [===>..........................] - ETA: 4s - loss: 0.0211 - accuracy: 0.9909\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 1117/5680 [====>.........................] - ETA: 4s - loss: 0.0219 - accuracy: 0.9907\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 1319/5680 [=====>........................] - ETA: 4s - loss: 0.0213 - accuracy: 0.9909\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 1465/5680 [======>.......................] - ETA: 4s - loss: 0.0210 - accuracy: 0.9911\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 5680/5680 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9912\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 2037/5680 [=========>....................] - ETA: 3s - loss: 0.0208 - accuracy: 0.9910\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2265/5680 [==========>...................] - ETA: 3s - loss: 0.0199 - accuracy: 0.9911\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2438/5680 [===========>..................] - ETA: 3s - loss: 0.0202 - accuracy: 0.9910\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2605/5680 [============>.................] - ETA: 3s - loss: 0.0198 - accuracy: 0.9911\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 2802/5680 [=============>................] - ETA: 2s - loss: 0.0201 - accuracy: 0.9911\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 3015/5680 [==============>...............] - ETA: 2s - loss: 0.0198 - accuracy: 0.9911\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 3577/5680 [=================>............] - ETA: 2s - loss: 0.0198 - accuracy: 0.9912\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 3773/5680 [==================>...........] - ETA: 1s - loss: 0.0197 - accuracy: 0.9913\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 3965/5680 [===================>..........] - ETA: 1s - loss: 0.0195 - accuracy: 0.9914\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 4165/5680 [====================>.........] - ETA: 1s - loss: 0.0195 - accuracy: 0.9914\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m Epoch 3/5\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 4314/5680 [=====================>........] - ETA: 1s - loss: 0.0195 - accuracy: 0.9914\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4526/5680 [======================>.......] - ETA: 1s - loss: 0.0208 - accuracy: 0.9908\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m \u001b[32m [repeated 409x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  150/5680 [..............................] - ETA: 5s - loss: 0.0199 - accuracy: 0.9908\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1686/5680 [=======>......................] - ETA: 4s - loss: 0.0179 - accuracy: 0.9917\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 3332/5680 [================>.............] - ETA: 2s - loss: 0.0180 - accuracy: 0.9917\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 3143/5680 [===============>..............] - ETA: 2s - loss: 0.0185 - accuracy: 0.9917\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4913/5680 [========================>.....] - ETA: 0s - loss: 0.0206 - accuracy: 0.9909\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4720/5680 [=======================>......] - ETA: 1s - loss: 0.0206 - accuracy: 0.9909\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5680/5680 [==============================] - 6s 1ms/step - loss: 0.0204 - accuracy: 0.9909\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 5288/5680 [==========================>...] - ETA: 0s - loss: 0.0197 - accuracy: 0.9912\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5101/5680 [=========================>....] - ETA: 0s - loss: 0.0205 - accuracy: 0.9909\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  336/5680 [>.............................] - ETA: 5s - loss: 0.0198 - accuracy: 0.9907\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1862/5680 [========>.....................] - ETA: 4s - loss: 0.0186 - accuracy: 0.9917\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  525/5680 [=>............................] - ETA: 5s - loss: 0.0196 - accuracy: 0.9910\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5473/5680 [===========================>..] - ETA: 0s - loss: 0.0205 - accuracy: 0.9908\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5658/5680 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9909\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  714/5680 [==>...........................] - ETA: 5s - loss: 0.0186 - accuracy: 0.9911\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  907/5680 [===>..........................] - ETA: 5s - loss: 0.0186 - accuracy: 0.9912\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1100/5680 [====>.........................] - ETA: 4s - loss: 0.0185 - accuracy: 0.9912\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1287/5680 [=====>........................] - ETA: 4s - loss: 0.0180 - accuracy: 0.9916\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1476/5680 [======>.......................] - ETA: 4s - loss: 0.0180 - accuracy: 0.9917\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2052/5680 [=========>....................] - ETA: 3s - loss: 0.0185 - accuracy: 0.9916\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 2231/5680 [==========>...................] - ETA: 3s - loss: 0.0180 - accuracy: 0.9918\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2442/5680 [===========>..................] - ETA: 3s - loss: 0.0184 - accuracy: 0.9914\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2627/5680 [============>.................] - ETA: 3s - loss: 0.0186 - accuracy: 0.9914\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2814/5680 [=============>................] - ETA: 3s - loss: 0.0186 - accuracy: 0.9913\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3000/5680 [==============>...............] - ETA: 2s - loss: 0.0186 - accuracy: 0.9914\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3573/5680 [=================>............] - ETA: 2s - loss: 0.0185 - accuracy: 0.9914\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3760/5680 [==================>...........] - ETA: 2s - loss: 0.0185 - accuracy: 0.9913\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3947/5680 [===================>..........] - ETA: 1s - loss: 0.0186 - accuracy: 0.9913\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=30476)\u001b[0m 4155/5680 [====================>.........] - ETA: 1s - loss: 0.0179 - accuracy: 0.9919\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m Epoch 4/5\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4324/5680 [=====================>........] - ETA: 1s - loss: 0.0187 - accuracy: 0.9912\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4515/5680 [======================>.......] - ETA: 1s - loss: 0.0187 - accuracy: 0.9912\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m \u001b[32m [repeated 349x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  146/5680 [..............................] - ETA: 5s - loss: 0.0147 - accuracy: 0.9927\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1702/5680 [=======>......................] - ETA: 4s - loss: 0.0181 - accuracy: 0.9914\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3392/5680 [================>.............] - ETA: 2s - loss: 0.0185 - accuracy: 0.9915\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3217/5680 [===============>..............] - ETA: 2s - loss: 0.0184 - accuracy: 0.9915\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4903/5680 [========================>.....] - ETA: 0s - loss: 0.0186 - accuracy: 0.9912\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4710/5680 [=======================>......] - ETA: 1s - loss: 0.0187 - accuracy: 0.9912\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5680/5680 [==============================] - 6s 1ms/step - loss: 0.0185 - accuracy: 0.9914\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5296/5680 [==========================>...] - ETA: 0s - loss: 0.0185 - accuracy: 0.9913\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5100/5680 [=========================>....] - ETA: 0s - loss: 0.0185 - accuracy: 0.9913\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  352/5680 [>.............................] - ETA: 6s - loss: 0.0173 - accuracy: 0.9917\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1857/5680 [========>.....................] - ETA: 3s - loss: 0.0180 - accuracy: 0.9916\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  541/5680 [=>............................] - ETA: 5s - loss: 0.0181 - accuracy: 0.9913\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5485/5680 [===========================>..] - ETA: 0s - loss: 0.0184 - accuracy: 0.9914\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5638/5680 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9914\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m  739/5680 [==>...........................] - ETA: 5s - loss: 0.0175 - accuracy: 0.9918\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  932/5680 [===>..........................] - ETA: 5s - loss: 0.0177 - accuracy: 0.9917\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1088/5680 [====>.........................] - ETA: 4s - loss: 0.0182 - accuracy: 0.9915\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1291/5680 [=====>........................] - ETA: 4s - loss: 0.0184 - accuracy: 0.9913\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 1498/5680 [======>.......................] - ETA: 4s - loss: 0.0180 - accuracy: 0.9919\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2067/5680 [=========>....................] - ETA: 3s - loss: 0.0179 - accuracy: 0.9916\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2270/5680 [==========>...................] - ETA: 3s - loss: 0.0182 - accuracy: 0.9918\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 2441/5680 [===========>..................] - ETA: 3s - loss: 0.0172 - accuracy: 0.9924\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2626/5680 [============>.................] - ETA: 3s - loss: 0.0182 - accuracy: 0.9917\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2830/5680 [=============>................] - ETA: 2s - loss: 0.0180 - accuracy: 0.9917\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2983/5680 [==============>...............] - ETA: 2s - loss: 0.0178 - accuracy: 0.9918\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 5680/5680 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9918\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3595/5680 [=================>............] - ETA: 2s - loss: 0.0181 - accuracy: 0.9916\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3749/5680 [==================>...........] - ETA: 1s - loss: 0.0180 - accuracy: 0.9918\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3950/5680 [===================>..........] - ETA: 1s - loss: 0.0178 - accuracy: 0.9918\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4144/5680 [====================>.........] - ETA: 1s - loss: 0.0180 - accuracy: 0.9918\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m Epoch 5/5\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4334/5680 [=====================>........] - ETA: 1s - loss: 0.0179 - accuracy: 0.9918\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4529/5680 [======================>.......] - ETA: 1s - loss: 0.0179 - accuracy: 0.9918\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m \u001b[32m [repeated 223x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  159/5680 [..............................] - ETA: 5s - loss: 0.0193 - accuracy: 0.9914\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m 1655/5680 [=======>......................] - ETA: 3s - loss: 0.0176 - accuracy: 0.9924\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3398/5680 [================>.............] - ETA: 2s - loss: 0.0180 - accuracy: 0.9916\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3189/5680 [===============>..............] - ETA: 2s - loss: 0.0180 - accuracy: 0.9917\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4875/5680 [========================>.....] - ETA: 0s - loss: 0.0177 - accuracy: 0.9918\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 4724/5680 [=======================>......] - ETA: 0s - loss: 0.0179 - accuracy: 0.9918\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5680/5680 [==============================] - 6s 1ms/step - loss: 0.0178 - accuracy: 0.9918\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5280/5680 [==========================>...] - ETA: 0s - loss: 0.0178 - accuracy: 0.9918\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5078/5680 [=========================>....] - ETA: 0s - loss: 0.0177 - accuracy: 0.9919\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  372/5680 [>.............................] - ETA: 5s - loss: 0.0186 - accuracy: 0.9919\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1879/5680 [========>.....................] - ETA: 3s - loss: 0.0167 - accuracy: 0.9924\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  527/5680 [=>............................] - ETA: 4s - loss: 0.0174 - accuracy: 0.9922\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5439/5680 [===========================>..] - ETA: 0s - loss: 0.0178 - accuracy: 0.9918\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 5646/5680 [============================>.] - ETA: 0s - loss: 0.0178 - accuracy: 0.9918\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  735/5680 [==>...........................] - ETA: 4s - loss: 0.0183 - accuracy: 0.9917\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m  940/5680 [===>..........................] - ETA: 4s - loss: 0.0176 - accuracy: 0.9919\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1098/5680 [====>.........................] - ETA: 4s - loss: 0.0175 - accuracy: 0.9920\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1297/5680 [=====>........................] - ETA: 4s - loss: 0.0170 - accuracy: 0.9921\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 1482/5680 [======>.......................] - ETA: 4s - loss: 0.0169 - accuracy: 0.9923\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2036/5680 [=========>....................] - ETA: 3s - loss: 0.0167 - accuracy: 0.9924\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2242/5680 [==========>...................] - ETA: 3s - loss: 0.0169 - accuracy: 0.9923\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2450/5680 [===========>..................] - ETA: 3s - loss: 0.0170 - accuracy: 0.9922\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2607/5680 [============>.................] - ETA: 3s - loss: 0.0169 - accuracy: 0.9923\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 2813/5680 [=============>................] - ETA: 2s - loss: 0.0167 - accuracy: 0.9923\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3020/5680 [==============>...............] - ETA: 2s - loss: 0.0166 - accuracy: 0.9923\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31416)\u001b[0m 4885/5680 [========================>.....] - ETA: 0s - loss: 0.0172 - accuracy: 0.9923\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3544/5680 [=================>............] - ETA: 2s - loss: 0.0165 - accuracy: 0.9925\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3749/5680 [==================>...........] - ETA: 1s - loss: 0.0167 - accuracy: 0.9924\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31720)\u001b[0m Client  5 Training complete...\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=31920)\u001b[0m 3959/5680 [===================>..........] - ETA: 1s - loss: 0.0167 - accuracy: 0.9924\u001b[32m [repeated 19x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-07-15 13:52:12,925 | server.py:232 | fit_round 1 received 10 results and 0 failures\n",
      "WARNING flwr 2023-07-15 13:52:12,939 | fedavg.py:243 | No fit_metrics_aggregation_fn provided\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server Evaluating... Evaluation Count:1\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print (f\"{Colours.YELLOW.value}\\nDeploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n",
    "print (f\"Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
    "print (f\"Writing output to: {sub_dir_name}/{test_directory_name}\\n{Colours.NORMAL.value}\")\n",
    "\n",
    "# Output the same information to the Output/Run_details.txt file\n",
    "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n",
    "    f.write(f\"{datetime.datetime.now()} - Deploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
    "\n",
    "    # Write Original train_df size\n",
    "    f.write(f\"{datetime.datetime.now()} - Original train_df size: {train_df_shape}\\n\")\n",
    "\n",
    "    # Write the training data split groups\n",
    "    for i in range(len(fl_X_train)):\n",
    "        f.write(f\"{datetime.datetime.now()} - {i}: X Shape {fl_X_train[i].shape}, Y Shape {fl_y_train[i].shape}\\n\")\n",
    "\n",
    "    # Write the testing data\n",
    "    f.write(f\"{datetime.datetime.now()} - X_test size: {X_test.shape}\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - y_test size: {y_test.shape}\\n\")\n",
    "    \n",
    "# close the file\n",
    "f.close()\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_OF_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_OF_ROUNDS),\n",
    "    strategy=strategy,\n",
    ")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Total time taken: \", end_time - start_time)\n",
    "\n",
    "print (f\"{Colours.YELLOW.value} SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n",
    "print (f\"Number of Clients = {NUM_OF_CLIENTS}{Colours.NORMAL.value}\\n\")\n",
    "\n",
    "# Output the same information to the Output/Run_details.txt file\n",
    "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n",
    "    f.write(f\"{datetime.datetime.now()} - SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - Total time taken: {end_time - start_time}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
