{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the available types of federated learning.\n",
    "\n",
    " - 'STRATIFIED': Stratified sampling of the data. The data is split into a number of shards, and each shard is assigned to a client. The data is split in a stratified manner, meaning that the distribution of the labels is approximately the same in each shard.\n",
    " - 'MISSING_1_ATTACK' - Each client is assigned a shard of data, each shard is missing one of the attack labels. Other clients in the network are exposed to the attack label, but the specific client is not. This demonstrates the ability of federated learning to protect against unknown attacks.\n",
    " - '1_ATTACK_ONLY' - Each client is assigned a shard of data, each shard contains only one of the attack labels.\n",
    " - 'HALF_BENIGN_ONLY' - Half of the clients are exposed to Benign data only, the other half are exposed to all data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS SECTION NEEDS TO BE SET TO DETERMINE WHICH CONFIGURATION METHOD TO UTILISE\n",
    "\n",
    "SPLIT_AVAILABLE_METHODS = ['STRATIFIED','MISSING_1_ATTACK', '1_ATTACK_ONLY', 'HALF_BENIGN_ONLY' ]\n",
    "METHOD = 'STRATIFIED'\n",
    "NUM_OF_STRATIFIED_CLIENTS = 20  # only applies to stratified method\n",
    "NUM_OF_ROUNDS = 10              # Number of FL rounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above test method in conjunction with the below classification selection will determine the number of clients.\n",
    "\n",
    "EG: \n",
    "`STRATIFIED` with:\n",
    " - `ALL TYPES` - Results in `NUM_OF_STRATIFIED_CLIENTS` clients. Each client will have a stratified sample of the data.\n",
    "\n",
    "`MISSING_1_ATTACK` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 32 attack labels.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 6 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n",
    "\n",
    "`1_ATTACK_ONLY` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 1 attack label.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 1 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS MISSING_1_ATTACK for binary classifier\n",
    "\n",
    "`HALF_BENIGN_ONLY` with:\n",
    " - `individual_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 33 malicious attack labels.\n",
    " - `group_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 7 malicious attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS MISSING_1_ATTACK for binary classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_classifier = False\n",
    "group_classifier = True\n",
    "binary_classifier = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the defines for the dataframe columns and the attack labels and their mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from includes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install flwr[simulation] torch torchvision matplotlib sklearn openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from flwr.common import Metrics\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flwr 1.4.0\n",
      "numpy 1.24.2\n",
      "torch 1.13.1\n",
      "Training on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRECTORY = '../datasets/CICIoT2023/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either read the training pickle file if it exists, or process the dataset from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists, loading data...\n",
      "Training data loaded from pickle file.\n",
      "Training data size: (908660, 47)\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the file 'training_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
    "if os.path.isfile('training_data.pkl'):\n",
    "    print(\"File exists, loading data...\")\n",
    "    train_df = pd.read_pickle('training_data.pkl')\n",
    "    print(\"Training data loaded from pickle file.\")\n",
    "\n",
    "else:\n",
    "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "    df_sets.sort()\n",
    "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
    "\n",
    "    # Print the number of files in each set\n",
    "    print('Training sets: {}'.format(len(training_sets)))\n",
    "    print('Test sets: {}'.format(len(test_sets)))\n",
    "\n",
    "    # ######################\n",
    "    # # HACK TEMP CODE\n",
    "    # ######################\n",
    "    # # Set training_sets to the last entry of training_sets\n",
    "    # training_sets = training_sets[-33:]\n",
    "    # print(f\"HACK TO REPLICATE ORIGINAL AUTHORS CODE WITH ONE FILE TRAIN - {training_sets}\")\n",
    "    # #####################\n",
    "    # # HACK END TEMP CODE\n",
    "    # ######################\n",
    "\n",
    "    # Concatenate all training sets into one dataframe\n",
    "    dfs = []\n",
    "    print(\"Reading training data...\")\n",
    "    for train_set in tqdm(training_sets):\n",
    "        df_new = pd.read_csv(DATASET_DIRECTORY + train_set)\n",
    "        dfs.append(df_new)\n",
    "    train_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Complete training data set size\n",
    "    print(\"Complete training data size: {}\".format(train_df.shape))\n",
    "\n",
    "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
    "    train_df['label'] = train_df['label'].map(dict_34_classes)\n",
    "\n",
    "    # The training data is the 80% of the CSV files in the dataset. The test data is the remaining 20%.\n",
    "    # The Ray Federated learning mechanism cannot cope with all of the 80% training data, so we will split\n",
    "    # the training data using test_train_split. The test data will be ignored as we will use all the data \n",
    "    # from the train_sets files as our training data to keep parity with the original authors code.\n",
    "    # \n",
    "    # By using a subset of the training data split this way, we can have a randomised selection of data\n",
    "    # from all the training CSV files, stratified by the attack types.\n",
    "    \n",
    "    # Percentage of original training data to use.\n",
    "    TRAIN_SIZE = 0.025\n",
    "    \n",
    "    print(f\"Splitting the data into {TRAIN_SIZE*100}%\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_df[X_columns], train_df[y_column], test_size= (1 - TRAIN_SIZE), random_state=42, stratify=train_df[y_column])\n",
    "\n",
    "    # Recombine X_train, and y_train into a dataframe\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # Clean up unused variables\n",
    "\n",
    "    del X_train, y_train, X_test, y_test\n",
    "    \n",
    "    # Save the output to a pickle file\n",
    "    print(\"Writing training data to pickle file...\")\n",
    "    train_df.to_pickle('training_data.pkl')\n",
    "\n",
    "print(\"Training data size: {}\".format(train_df.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of attacks in train_df:\n",
      "6     140143\n",
      "4     105397\n",
      "5      87536\n",
      "2      79671\n",
      "3      78959\n",
      "1      78729\n",
      "7      70063\n",
      "13     64601\n",
      "15     51987\n",
      "14     39474\n",
      "0      21372\n",
      "17     19294\n",
      "19     17330\n",
      "18     14637\n",
      "10      8794\n",
      "26      5990\n",
      "9       5595\n",
      "8       5560\n",
      "25      3478\n",
      "24      2615\n",
      "21      1914\n",
      "22      1596\n",
      "16      1403\n",
      "23       729\n",
      "12       559\n",
      "11       454\n",
      "33       252\n",
      "27       115\n",
      "32       106\n",
      "31       102\n",
      "29        74\n",
      "28        62\n",
      "20        44\n",
      "30        25\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# show the unique values counts in the label column for train_df\n",
    "print(\"Counts of attacks in train_df:\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>Header_Length</th>\n",
       "      <th>Protocol Type</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Srate</th>\n",
       "      <th>Drate</th>\n",
       "      <th>fin_flag_number</th>\n",
       "      <th>syn_flag_number</th>\n",
       "      <th>rst_flag_number</th>\n",
       "      <th>...</th>\n",
       "      <th>Std</th>\n",
       "      <th>Tot size</th>\n",
       "      <th>IAT</th>\n",
       "      <th>Number</th>\n",
       "      <th>Magnitue</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Covariance</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Weight</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13070266</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>60.072672</td>\n",
       "      <td>60.072672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>8.312483e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.165151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111392</th>\n",
       "      <td>0.032670</td>\n",
       "      <td>162.67</td>\n",
       "      <td>16.89</td>\n",
       "      <td>65.91</td>\n",
       "      <td>0.685361</td>\n",
       "      <td>0.685361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.433546</td>\n",
       "      <td>160.92</td>\n",
       "      <td>8.301119e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>17.761121</td>\n",
       "      <td>9.106583</td>\n",
       "      <td>233.739318</td>\n",
       "      <td>0.19</td>\n",
       "      <td>141.55</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14955759</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>61.182930</td>\n",
       "      <td>61.182930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>8.315025e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.165151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245562</th>\n",
       "      <td>0.092809</td>\n",
       "      <td>63.12</td>\n",
       "      <td>5.94</td>\n",
       "      <td>62.90</td>\n",
       "      <td>1.437224</td>\n",
       "      <td>1.437224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403326</td>\n",
       "      <td>54.10</td>\n",
       "      <td>8.331381e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.408309</td>\n",
       "      <td>0.571145</td>\n",
       "      <td>0.764002</td>\n",
       "      <td>0.24</td>\n",
       "      <td>141.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20485085</th>\n",
       "      <td>2.581335</td>\n",
       "      <td>228.96</td>\n",
       "      <td>6.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>1.754608</td>\n",
       "      <td>1.754608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00</td>\n",
       "      <td>8.336144e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.392305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28314625</th>\n",
       "      <td>0.000624</td>\n",
       "      <td>54.86</td>\n",
       "      <td>6.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>144.071746</td>\n",
       "      <td>144.071746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997117</td>\n",
       "      <td>54.20</td>\n",
       "      <td>8.309440e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.425904</td>\n",
       "      <td>1.411650</td>\n",
       "      <td>6.126863</td>\n",
       "      <td>0.17</td>\n",
       "      <td>141.55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25072079</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>162.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>45.678888</td>\n",
       "      <td>45.678888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>162.00</td>\n",
       "      <td>8.300816e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19742459</th>\n",
       "      <td>0.057148</td>\n",
       "      <td>17765.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>12286.507156</td>\n",
       "      <td>12286.507156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00</td>\n",
       "      <td>8.310670e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27891842</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.23</td>\n",
       "      <td>6.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>2.046859</td>\n",
       "      <td>2.046859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340412</td>\n",
       "      <td>55.57</td>\n",
       "      <td>8.294319e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.424976</td>\n",
       "      <td>1.906483</td>\n",
       "      <td>43.601168</td>\n",
       "      <td>0.05</td>\n",
       "      <td>141.55</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198067</th>\n",
       "      <td>0.030795</td>\n",
       "      <td>69.36</td>\n",
       "      <td>6.00</td>\n",
       "      <td>63.81</td>\n",
       "      <td>5.414858</td>\n",
       "      <td>5.414858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.483812</td>\n",
       "      <td>55.61</td>\n",
       "      <td>8.294730e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.505923</td>\n",
       "      <td>6.353254</td>\n",
       "      <td>185.267078</td>\n",
       "      <td>0.11</td>\n",
       "      <td>141.55</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>908660 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          flow_duration  Header_Length  Protocol Type  Duration          Rate  \\\n",
       "13070266       0.000000           0.00           1.00     64.00     60.072672   \n",
       "11111392       0.032670         162.67          16.89     65.91      0.685361   \n",
       "14955759       0.000000           0.00           1.00     64.00     61.182930   \n",
       "3245562        0.092809          63.12           5.94     62.90      1.437224   \n",
       "20485085       2.581335         228.96           6.00     64.00      1.754608   \n",
       "...                 ...            ...            ...       ...           ...   \n",
       "28314625       0.000624          54.86           6.00     64.00    144.071746   \n",
       "25072079       0.000000         162.00          17.00     64.00     45.678888   \n",
       "19742459       0.057148       17765.00          17.00     64.00  12286.507156   \n",
       "27891842       0.000000          56.23           6.00     64.00      2.046859   \n",
       "2198067        0.030795          69.36           6.00     63.81      5.414858   \n",
       "\n",
       "                 Srate  Drate  fin_flag_number  syn_flag_number  \\\n",
       "13070266     60.072672    0.0              0.0              0.0   \n",
       "11111392      0.685361    0.0              0.0              0.0   \n",
       "14955759     61.182930    0.0              0.0              0.0   \n",
       "3245562       1.437224    0.0              0.0              0.0   \n",
       "20485085      1.754608    0.0              0.0              1.0   \n",
       "...                ...    ...              ...              ...   \n",
       "28314625    144.071746    0.0              0.0              1.0   \n",
       "25072079     45.678888    0.0              0.0              0.0   \n",
       "19742459  12286.507156    0.0              0.0              0.0   \n",
       "27891842      2.046859    0.0              0.0              0.0   \n",
       "2198067       5.414858    0.0              0.0              0.0   \n",
       "\n",
       "          rst_flag_number  ...       Std  Tot size           IAT  Number  \\\n",
       "13070266              0.0  ...  0.000000     42.00  8.312483e+07     9.5   \n",
       "11111392              0.0  ...  6.433546    160.92  8.301119e+07     9.5   \n",
       "14955759              0.0  ...  0.000000     42.00  8.315025e+07     9.5   \n",
       "3245562               0.0  ...  0.403326     54.10  8.331381e+07     9.5   \n",
       "20485085              0.0  ...  0.000000     54.00  8.336144e+07     9.5   \n",
       "...                   ...  ...       ...       ...           ...     ...   \n",
       "28314625              0.0  ...  0.997117     54.20  8.309440e+07     9.5   \n",
       "25072079              0.0  ...  0.000000    162.00  8.300816e+07     9.5   \n",
       "19742459              0.0  ...  0.000000     50.00  8.310670e+07     9.5   \n",
       "27891842              0.0  ...  1.340412     55.57  8.294319e+07     9.5   \n",
       "2198067               0.0  ...  4.483812     55.61  8.294730e+07     9.5   \n",
       "\n",
       "           Magnitue    Radius  Covariance  Variance  Weight  label  \n",
       "13070266   9.165151  0.000000    0.000000      0.00  141.55      6  \n",
       "11111392  17.761121  9.106583  233.739318      0.19  141.55     13  \n",
       "14955759   9.165151  0.000000    0.000000      0.00  141.55      6  \n",
       "3245562   10.408309  0.571145    0.764002      0.24  141.55      2  \n",
       "20485085  10.392305  0.000000    0.000000      0.00  141.55      7  \n",
       "...             ...       ...         ...       ...     ...    ...  \n",
       "28314625  10.425904  1.411650    6.126863      0.17  141.55      3  \n",
       "25072079  18.000000  0.000000    0.000000      0.00  141.55     13  \n",
       "19742459  10.000000  0.000000    0.000000      0.00  141.55      4  \n",
       "27891842  10.424976  1.906483   43.601168      0.05  141.55     15  \n",
       "2198067   10.505923  6.353254  185.267078      0.11  141.55     15  \n",
       "\n",
       "[908660 rows x 47 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Data\n",
    "Concat the test data into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File testing_data.pkl exists, loading data...\n",
      "Test data loaded from pickle file.\n",
      "Testing data size: (10340161, 47)\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the file 'test_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
    "testing_data_pickle_file = 'testing_data.pkl'\n",
    "\n",
    "if os.path.isfile(testing_data_pickle_file):\n",
    "    print(f\"File {testing_data_pickle_file} exists, loading data...\")\n",
    "    test_df = pd.read_pickle(testing_data_pickle_file)\n",
    "    print(\"Test data loaded from pickle file.\")\n",
    "\n",
    "else:\n",
    "    print(f\"File {testing_data_pickle_file} does not exist, constructing data...\")\n",
    "\n",
    "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "    df_sets.sort()\n",
    "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
    "\n",
    "    ############################################\n",
    "    ############################################\n",
    "    # HACK - Make things quicker for now\n",
    "    ############################################\n",
    "    ############################################\n",
    "\n",
    "    # test_sets = df_sets[int(len(df_sets)*.95):]\n",
    "    \n",
    "    # # Set training_sets to the last entry of training_sets\n",
    "    # test_sets = test_sets[-2:]\n",
    "    \n",
    "    ############################################\n",
    "    ############################################\n",
    "    # END HACK \n",
    "    ############################################\n",
    "    ############################################\n",
    "\n",
    "    # Print the number of files in each set\n",
    "    print('Test sets: {}'.format(len(test_sets)))\n",
    "    \n",
    "    # Concatenate all testing sets into one dataframe\n",
    "    dfs = []\n",
    "    print(\"Reading test data...\")\n",
    "    for test_set in tqdm(test_sets):\n",
    "        df_new = pd.read_csv(DATASET_DIRECTORY + test_set)\n",
    "        dfs.append(df_new)\n",
    "    test_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
    "    test_df['label'] = test_df['label'].map(dict_34_classes)\n",
    "\n",
    "    # Save the output to a pickle file\n",
    "    print(f\"Writing test data to pickle file {testing_data_pickle_file}...\")\n",
    "    test_df.to_pickle(testing_data_pickle_file)\n",
    "\n",
    "print(\"Testing data size: {}\".format(test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train_df: 908660\n",
      "Number of rows in test_df: 10340161\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in train_df: {}\".format(len(train_df)))\n",
    "print(\"Number of rows in test_df: {}\".format(len(test_df)))\n",
    "\n",
    "train_size = len(train_df)\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Scale the test and train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the training data input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df[X_columns] = scaler.fit_transform(train_df[X_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the testing data input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[X_columns] = scaler.fit_transform(test_df[X_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define the classification problem - (2 classes, 8 classes or 34 classes)\n",
    "Change the following cell to select the classification type\n",
    "\n",
    "If the METHOD == STRATIFIED, then we can use any classifier\n",
    "If the METHOD == ATTACK_GROUP then we must use Group Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 8 Class Classifier... - Adjusting labels in test and train dataframes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_size_map = {2: \"Binary\", 8: \"Group\", 34: \"Individual\"}\n",
    "\n",
    "if group_classifier:\n",
    "    print(\"Group 8 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
    "    # Map y column to the dict_7_classes values\n",
    "    test_df['label'] = test_df['label'].map(dict_8_classes)\n",
    "    train_df['label'] = train_df['label'].map(dict_8_classes)\n",
    "    class_size = \"8\"      \n",
    "    \n",
    "elif binary_classifier:\n",
    "    print(\"Binary 2 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
    "    # Map y column to the dict_2_classes values\n",
    "    test_df['label'] = test_df['label'].map(dict_2_classes)\n",
    "    train_df['label'] = train_df['label'].map(dict_2_classes)\n",
    "    class_size = \"2\"\n",
    "\n",
    "else:\n",
    "    print (\"Individual 34 Class classifier... - No adjustments to labels in test and train dataframes\")\n",
    "    class_size = \"34\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Split the Training Data into partitions for the Federated Learning clients depending on the test required\n",
    "As a reminder:\n",
    "\n",
    "`STRATIFIED` with:\n",
    " - `ALL TYPES` - Results in `NUM_OF_STRATIFIED_CLIENTS` clients. Each client will have a stratified sample of the data.\n",
    "\n",
    "`MISSING_1_ATTACK` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 32 attack labels.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 6 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n",
    "\n",
    "`1_ATTACK_ONLY` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 1 attack label.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 1 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS MISSING_1_ATTACK for binary classifier\n",
    "\n",
    "`HALF_BENIGN_ONLY` with:\n",
    " - `individual_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 33 malicious attack labels.\n",
    " - `group_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 7 malicious attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS MISSING_1_ATTACK for binary classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mSTRATIFIED METHOD\u001b[0m with 8 class classifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define fl_X_train and fl_y_train\n",
    "fl_X_train = []\n",
    "fl_y_train = []\n",
    "\n",
    "client_df = pd.DataFrame()\n",
    "\n",
    "if METHOD == 'STRATIFIED':\n",
    "    print(f\"{Colours.YELLOW.value}STRATIFIED METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "    # We are going to split the training data into 'NUM_OF_STRATIFIED_CLIENTS' smaller groups using StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=NUM_OF_STRATIFIED_CLIENTS, shuffle=True, random_state=42)\n",
    "    for train_index, test_index in skf.split(train_df[X_columns], train_df[y_column]):\n",
    "        fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "        fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "\n",
    "elif METHOD == 'MISSING_1_ATTACK':\n",
    "    print(f\"{Colours.YELLOW.value}MISSING_1_ATTACK METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "\n",
    "    if individual_classifier or group_classifier:\n",
    "        # Set the number of splits required to the number of classes - 1\n",
    "        num_splits = int(class_size) - 1\n",
    "    else:\n",
    "        # For binary classifier, set the number of splits to 10\n",
    "        num_splits = 10\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # When creating the clients, we will remove one attack class from the training data\n",
    "    # For the binary classifier, evey other client will have the benign class removed\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
    "        if binary_classifier:\n",
    "            print(f\"i: {i} = i % 2 = {i % 2}\")\n",
    "            if i % 2 == 0:\n",
    "                print(\"Benign only\")\n",
    "                # Create a new dataframe for the client data with only benign traffic\n",
    "                client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != 1]], ignore_index=True)\n",
    "                fl_X_train.append(client_df[X_columns])\n",
    "                fl_y_train.append(client_df[y_column])\n",
    "            else:\n",
    "                print(\"Both\")\n",
    "                # Create a new dataframe for the client data\n",
    "                fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "                fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "        else:\n",
    "            # Create a new dataframe for the client data\n",
    "            client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != i+1]], ignore_index=True)\n",
    "            fl_X_train.append(client_df[X_columns])\n",
    "            fl_y_train.append(client_df[y_column])\n",
    "\n",
    "elif METHOD == '1_ATTACK_ONLY':\n",
    "    print(f\"{Colours.YELLOW.value}1_ATTACK_ONLY METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "    # Each client only has one attack class in their training data along with the Benign data\n",
    "    \n",
    "    if individual_classifier or group_classifier:\n",
    "        # Set the number of splits required to the number of classes - 1\n",
    "        num_splits = int(class_size) - 1\n",
    "    else:\n",
    "        # For binary classifier, set the number of splits to 10\n",
    "        num_splits = 10\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # When creating the clients, we will only add the benign data and the attack class for that client\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
    "        if binary_classifier:\n",
    "            print(f\"i: {i} = i % 2 = {i % 2}\")\n",
    "            if i % 2 == 0:\n",
    "                print(\"Benign only\")\n",
    "                # Create a new dataframe for the client data with only benign traffic\n",
    "                client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != 1]], ignore_index=True)\n",
    "                fl_X_train.append(client_df[X_columns])\n",
    "                fl_y_train.append(client_df[y_column])\n",
    "            else:\n",
    "                print(\"Both\")\n",
    "                # Create a new dataframe for the client data\n",
    "                fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "                fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "        else:\n",
    "            # Create a new dataframe for the client data\n",
    "            client_df = pd.concat([train_df.iloc[test_index][(train_df[y_column] == 0) | (train_df[y_column] == i+1)]], ignore_index=True)\n",
    "            fl_X_train.append(client_df[X_columns])\n",
    "            fl_y_train.append(client_df[y_column])\n",
    "\n",
    "elif METHOD == 'HALF_BENIGN_ONLY':\n",
    "    print(f\"{Colours.YELLOW.value}HALF_BENIGN_ONLY METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "\n",
    "    num_splits = 10\n",
    "\n",
    "    # Split into 10 client data\n",
    "    skf = StratifiedKFold(n_splits=NUM_OF_STRATIFIED_CLIENTS, shuffle=True, random_state=42)\n",
    "\n",
    "    # For i % 2 == 0, add only benign data\n",
    "    # For i % 2 == 1, add all data\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
    "        if i % 2 == 0:\n",
    "            print(\"Benign only\")\n",
    "            # Create a new dataframe for the client data with only benign traffic\n",
    "            client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n",
    "            fl_X_train.append(client_df[X_columns])\n",
    "            fl_y_train.append(client_df[y_column])\n",
    "        else:\n",
    "            print(\"All Classes\")\n",
    "            fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "            fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "else:\n",
    "    print(f\"{Colours.RED.value}ERROR: Method {METHOD} not recognised{Colours.NORMAL.value}\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fl_X_train[0].shape: (45433, 46)\n",
      "fl_y_train[0].value_counts():\n",
      "1    33073\n",
      "7     7874\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       12\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[0].unique(): [1 7 0 2 5 3 4 6]\n",
      "\n",
      "fl_X_train[1].shape: (45433, 46)\n",
      "fl_y_train[1].value_counts():\n",
      "1    33073\n",
      "7     7874\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       12\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[1].unique(): [1 7 0 2 4 3 5 6]\n",
      "\n",
      "fl_X_train[2].shape: (45433, 46)\n",
      "fl_y_train[2].value_counts():\n",
      "1    33073\n",
      "7     7874\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       12\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[2].unique(): [7 3 1 2 0 4 5 6]\n",
      "\n",
      "fl_X_train[3].shape: (45433, 46)\n",
      "fl_y_train[3].value_counts():\n",
      "1    33073\n",
      "7     7874\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       12\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[3].unique(): [1 3 2 7 0 4 5 6]\n",
      "\n",
      "fl_X_train[4].shape: (45433, 46)\n",
      "fl_y_train[4].value_counts():\n",
      "1    33073\n",
      "7     7874\n",
      "2     2563\n",
      "0     1068\n",
      "4      474\n",
      "3      344\n",
      "5       25\n",
      "6       12\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[4].unique(): [1 2 0 3 7 4 6 5]\n",
      "\n",
      "fl_X_train[5].shape: (45433, 46)\n",
      "fl_y_train[5].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2564\n",
      "0     1068\n",
      "4      474\n",
      "3      344\n",
      "5       25\n",
      "6       12\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[5].unique(): [2 1 7 4 0 3 6 5]\n",
      "\n",
      "fl_X_train[6].shape: (45433, 46)\n",
      "fl_y_train[6].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1068\n",
      "4      474\n",
      "3      345\n",
      "5       25\n",
      "6       12\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[6].unique(): [1 7 2 3 0 4 5 6]\n",
      "\n",
      "fl_X_train[7].shape: (45433, 46)\n",
      "fl_y_train[7].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1068\n",
      "4      474\n",
      "3      345\n",
      "5       25\n",
      "6       12\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[7].unique(): [4 1 7 0 2 3 6 5]\n",
      "\n",
      "fl_X_train[8].shape: (45433, 46)\n",
      "fl_y_train[8].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1068\n",
      "4      474\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[8].unique(): [1 7 0 2 3 4 5 6]\n",
      "\n",
      "fl_X_train[9].shape: (45433, 46)\n",
      "fl_y_train[9].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1068\n",
      "4      474\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[9].unique(): [7 1 0 2 4 3 5 6]\n",
      "\n",
      "fl_X_train[10].shape: (45433, 46)\n",
      "fl_y_train[10].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1068\n",
      "4      474\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[10].unique(): [1 4 2 7 0 3 6 5]\n",
      "\n",
      "fl_X_train[11].shape: (45433, 46)\n",
      "fl_y_train[11].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1068\n",
      "4      474\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[11].unique(): [1 0 7 2 3 5 6 4]\n",
      "\n",
      "fl_X_train[12].shape: (45433, 46)\n",
      "fl_y_train[12].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[12].unique(): [1 7 4 0 2 3 5 6]\n",
      "\n",
      "fl_X_train[13].shape: (45433, 46)\n",
      "fl_y_train[13].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[13].unique(): [1 2 7 4 0 3 5 6]\n",
      "\n",
      "fl_X_train[14].shape: (45433, 46)\n",
      "fl_y_train[14].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[14].unique(): [1 2 7 3 0 4 5 6]\n",
      "\n",
      "fl_X_train[15].shape: (45433, 46)\n",
      "fl_y_train[15].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[15].unique(): [1 7 2 0 4 5 3 6]\n",
      "\n",
      "fl_X_train[16].shape: (45433, 46)\n",
      "fl_y_train[16].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[16].unique(): [1 2 7 0 4 3 6 5]\n",
      "\n",
      "fl_X_train[17].shape: (45433, 46)\n",
      "fl_y_train[17].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[17].unique(): [7 1 2 4 0 3 5 6]\n",
      "\n",
      "fl_X_train[18].shape: (45433, 46)\n",
      "fl_y_train[18].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[18].unique(): [1 7 0 3 2 4 6 5]\n",
      "\n",
      "fl_X_train[19].shape: (45433, 46)\n",
      "fl_y_train[19].value_counts():\n",
      "1    33073\n",
      "7     7873\n",
      "2     2563\n",
      "0     1069\n",
      "4      473\n",
      "3      345\n",
      "5       24\n",
      "6       13\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[19].unique(): [1 7 2 4 3 0 5 6]\n",
      "\n",
      "fl_X_train[0].equals(fl_X_train[1]): False\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_CLIENTS = len(fl_X_train)\n",
    "\n",
    "for i in range(len(fl_X_train)):\n",
    "    # Show the unique values in the y column\n",
    "    (f\"Client ID: {i}\")\n",
    "    print(f\"fl_X_train[{i}].shape: {fl_X_train[i].shape}\")  \n",
    "    print(f\"fl_y_train[{i}].value_counts():\\n{fl_y_train[i].value_counts()}\")\n",
    "    print(f\"fl_y_train[{i}].unique(): {fl_y_train[i].unique()}\\n\")\n",
    "\n",
    "# Check that fl_X_train[0] and fl_X_train[1] contain different data\n",
    "print(f\"fl_X_train[0].equals(fl_X_train[1]): {fl_X_train[0].equals(fl_X_train[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare an output directory where we can store the results of the federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an \"Output\" directory if it doesnt exist already\n",
    "if not os.path.exists(\"Output\"):\n",
    "    os.makedirs(\"Output\")\n",
    "\n",
    "sub_dir_name = f\"train_size-{train_size}_test_size-{test_size}\"\n",
    "\n",
    "# if sub_dir_name does not exist, create it\n",
    "if not os.path.exists(f\"Output/{sub_dir_name}\"):\n",
    "    os.makedirs(f\"Output/{sub_dir_name}\")\n",
    "\n",
    "test_directory_name = f\"{METHOD}_Classifier-{class_size}_Clients-{NUM_OF_CLIENTS}\"\n",
    "\n",
    "# Create an \"Output/{METHOD}-{NUM_OF_CLIENTS}-{NUM_OF_ROUNDS}\" directory if it doesnt exist already\n",
    "if not os.path.exists(f\"Output/{sub_dir_name}/{test_directory_name}\"):\n",
    "    os.makedirs(f\"Output/{sub_dir_name}/{test_directory_name}\")\n",
    "\n",
    "# Ensure the directory is empty\n",
    "for file in os.listdir(f\"Output/{sub_dir_name}/{test_directory_name}\"):\n",
    "    file_path = os.path.join(f\"Output/{sub_dir_name}/{test_directory_name}\", file)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.unlink(file_path)\n",
    "\n",
    "# Original training size is the sum of all the fl_X_train sizes\n",
    "original_training_size = 0\n",
    "for i in range(len(fl_X_train)):\n",
    "    original_training_size += fl_X_train[i].shape[0]\n",
    "\n",
    "# Write this same info to the output directory/Class Split Info.txt\n",
    "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Class Split Info.txt\", \"w\") as f:\n",
    "    for i in range(len(fl_X_train)):\n",
    "        f.write(f\"Client ID: {i}\\n\")\n",
    "        f.write(f\"fl_X_train.shape: {fl_X_train[i].shape}\\n\")\n",
    "        f.write(f\"Training data used {original_training_size}\")\n",
    "        f.write(f\"fl_y_train.value_counts():\\n{fl_y_train[i].value_counts()}\\n\")\n",
    "        f.write(f\"fl_y_train.unique(): {fl_y_train[i].unique()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the testing daya to X_test and y_test ndarrays\n",
    "X_test = test_df[X_columns].to_numpy()\n",
    "y_test = test_df[y_column].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_classes = len(train_df[y_column].unique())\n",
    "\n",
    "train_df_shape = train_df.shape\n",
    "test_df_shape = test_df.shape\n",
    "\n",
    "# We are now done with the train_df and test_df dataframes, so we can delete them to free up memory\n",
    "del train_df\n",
    "del test_df\n",
    "del client_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CLIENTS: 20\n",
      "NUM_ROUNDS: 10\n",
      "\n",
      "Original training size: 908660\n",
      "Checking training data split groups\n",
      "0 : X Shape (45433, 46) Y Shape (45433,)\n",
      "1 : X Shape (45433, 46) Y Shape (45433,)\n",
      "2 : X Shape (45433, 46) Y Shape (45433,)\n",
      "3 : X Shape (45433, 46) Y Shape (45433,)\n",
      "4 : X Shape (45433, 46) Y Shape (45433,)\n",
      "5 : X Shape (45433, 46) Y Shape (45433,)\n",
      "6 : X Shape (45433, 46) Y Shape (45433,)\n",
      "7 : X Shape (45433, 46) Y Shape (45433,)\n",
      "8 : X Shape (45433, 46) Y Shape (45433,)\n",
      "9 : X Shape (45433, 46) Y Shape (45433,)\n",
      "10 : X Shape (45433, 46) Y Shape (45433,)\n",
      "11 : X Shape (45433, 46) Y Shape (45433,)\n",
      "12 : X Shape (45433, 46) Y Shape (45433,)\n",
      "13 : X Shape (45433, 46) Y Shape (45433,)\n",
      "14 : X Shape (45433, 46) Y Shape (45433,)\n",
      "15 : X Shape (45433, 46) Y Shape (45433,)\n",
      "16 : X Shape (45433, 46) Y Shape (45433,)\n",
      "17 : X Shape (45433, 46) Y Shape (45433,)\n",
      "18 : X Shape (45433, 46) Y Shape (45433,)\n",
      "19 : X Shape (45433, 46) Y Shape (45433,)\n",
      "\n",
      "Checking testing data\n",
      "X_test size: (10340161, 46)\n",
      "y_test size: (10340161,)\n",
      "\n",
      "Deploy Simulation\n"
     ]
    }
   ],
   "source": [
    "print(\"NUM_CLIENTS:\", NUM_OF_CLIENTS)\n",
    "\n",
    "print(\"NUM_ROUNDS:\", NUM_OF_ROUNDS)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Original training size: {}\".format(original_training_size))\n",
    "\n",
    "\n",
    "print(\"Checking training data split groups\")\n",
    "for i in range(len(fl_X_train)):\n",
    "    print(i, \":\", \"X Shape\", fl_X_train[i].shape, \"Y Shape\", fl_y_train[i].shape)\n",
    "\n",
    "\n",
    "# Print the sizes of X_test and y_test\n",
    "print(\"\\nChecking testing data\")\n",
    "print(\"X_test size: {}\".format(X_test.shape))\n",
    "print(\"y_test size: {}\".format(y_test.shape))\n",
    "\n",
    "print(\"\\nDeploy Simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Federated Learning\n",
    "## Import the libraries and print the versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Client and Server code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn 1.2.0.\n",
      "flwr 1.4.0\n",
      "numpy 1.24.2\n",
      "tf 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print('scikit-learn {}.'.format(sklearn.__version__))\n",
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"tf\", tf.__version__)\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "import datetime\n",
    "\n",
    "client_evaluations = []\n",
    "\n",
    "class NumpyFlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, model, train_data, train_labels):\n",
    "        self.model = model\n",
    "        self.cid = cid\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Training...\")\n",
    "        self.model.fit(self.train_data, self.train_labels, epochs=5, batch_size=32)\n",
    "        print (\"Client \", self.cid, \"Training complete...\")\n",
    "        return self.model.get_weights(), len(self.train_data), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Evaluating...\")\n",
    "        loss, accuracy = self.model.evaluate(self.train_data, self.train_labels, batch_size=32)\n",
    "        print(f\"{Colours.YELLOW.value}Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}{Colours.NORMAL.value}\")\n",
    "\n",
    "        # Write the same message to the \"Output/{cid}_Evaluation.txt\" file\n",
    "        with open(f\"Output/{sub_dir_name}/{test_directory_name}/{self.cid}_Evaluation.txt\", \"a\") as f:\n",
    "            f.write(f\"{datetime.datetime.now()} - Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}\\n\")\n",
    "\n",
    "            # Close the file\n",
    "            f.close()\n",
    "\n",
    "        return loss, len(self.train_data), {\"accuracy\": accuracy}\n",
    "    \n",
    "    def predict(self, incoming):\n",
    "        prediction = np.argmax( self.model.predict(incoming) ,axis=1)\n",
    "        return prediction\n",
    "\n",
    "def client_fn(cid: str) -> NumpyFlowerClient:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    #model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n",
    "    #model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    print (\"Client ID:\", cid)\n",
    "\n",
    "    model = Sequential([\n",
    "      #Flatten(input_shape=(79,1)),\n",
    "      Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "      Dense(50, activation='relu'),  \n",
    "      Dense(25, activation='relu'),  \n",
    "      Dense(num_unique_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "   \n",
    "    partition_id = int(cid)\n",
    "    X_train_c = fl_X_train[partition_id]\n",
    "    y_train_c = fl_y_train[partition_id]\n",
    "\n",
    "    # Create a  single Flower client representing a single organization\n",
    "    return NumpyFlowerClient(cid, model, X_train_c, y_train_c)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "eval_count = 0\n",
    "\n",
    "def get_evaluate_fn(server_model):\n",
    "    global eval_count\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "    # The `evaluate` function will be called after every round\n",
    "    \n",
    "    \n",
    "    def evaluate(server_round, parameters, config):\n",
    "        global eval_count\n",
    "        \n",
    "        # Update model with the latest parameters\n",
    "        server_model.set_weights(parameters)\n",
    "        print (f\"Server Evaluating... Evaluation Count:{eval_count}\")\n",
    "        loss, accuracy = server_model.evaluate(X_test, y_test)\n",
    "        \n",
    "        y_pred = server_model.predict(X_test)\n",
    "        print (\"Prediction: \", y_pred, y_pred.shape)\n",
    "        #cmatrix = confusion_matrix(y_test, np.rint(y_pred))\n",
    "        #print (\"confusion_matrix:\", cmatrix, cmatrix.shape)\n",
    "                        \n",
    "        print(f\"{Colours.YELLOW.value}Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}{Colours.NORMAL.value}\")\n",
    "\n",
    "        # Write the same message to the \"Output/Server_Evaluation.txt\" file\n",
    "        with open(f\"Output/{sub_dir_name}/{test_directory_name}/Server_Evaluation.txt\", \"a\") as f:\n",
    "            f.write(f\"{datetime.datetime.now()} - {server_round} : Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\\n\")\n",
    "\n",
    "            # Close the file\n",
    "            f.close()\n",
    "        \n",
    "        np.save(\"y_pred-\" + str(eval_count) + \".npy\", y_pred)\n",
    "        #np.save(\"cmatrix-\" + str(eval_count) + \".npy\", cmatrix)\n",
    "        eval_count = eval_count + 1\n",
    "        \n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "    return evaluate\n",
    "\n",
    "\n",
    "\n",
    "server_model = Sequential([\n",
    "    #Flatten(input_shape=(79,1)),\n",
    "    Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "    Dense(50, activation='relu'),  \n",
    "    Dense(25, activation='relu'),  \n",
    "    Dense(num_unique_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "server_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=2, #10,\n",
    "        min_evaluate_clients=2, #5,\n",
    "        min_available_clients=2, #10,\n",
    "        evaluate_fn=get_evaluate_fn(server_model),\n",
    "        #evaluate_metrics_aggregation_fn=weighted_average,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-07-13 17:44:02,968 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\n",
      "Deploy simulation... Method = STRATIFIED - Group (8) Classifier\n",
      "Number of Clients = 20\n",
      "\n",
      "Writing output to: train_size-908660_test_size-10340161/STRATIFIED_Classifier-8_Clients-20\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 17:44:09,309\tINFO worker.py:1636 -- Started a local Ray instance.\n",
      "INFO flwr 2023-07-13 17:44:13,914 | app.py:180 | Flower VCE: Ray initialized with resources: {'node:127.0.0.1': 1.0, 'object_store_memory': 16172711116.0, 'memory': 32345422235.0, 'CPU': 24.0, 'GPU': 1.0}\n",
      "INFO flwr 2023-07-13 17:44:13,915 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-07-13 17:44:13,915 | server.py:273 | Requesting initial parameters from one random client\n",
      "INFO flwr 2023-07-13 17:44:18,480 | server.py:277 | Received initial parameters from one random client\n",
      "INFO flwr 2023-07-13 17:44:18,481 | server.py:88 | Evaluating initial parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=19560)\u001b[0m Client ID: 3\n",
      "Server Evaluating... Evaluation Count:0\n",
      "323131/323131 [==============================] - 233s 721us/step - loss: 2.6138 - accuracy: 0.0019\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print (f\"{Colours.YELLOW.value}\\nDeploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n",
    "print (f\"Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
    "print (f\"Writing output to: {sub_dir_name}/{test_directory_name}\\n{Colours.NORMAL.value}\")\n",
    "\n",
    "# Output the same information to the Output/Run_details.txt file\n",
    "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n",
    "    f.write(f\"{datetime.datetime.now()} - Deploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
    "\n",
    "    # Write Original train_df size\n",
    "    f.write(f\"{datetime.datetime.now()} - Original train_df size: {train_df_shape}\\n\")\n",
    "\n",
    "    # Write the training data split groups\n",
    "    for i in range(len(fl_X_train)):\n",
    "        f.write(f\"{datetime.datetime.now()} - {i}: X Shape {fl_X_train[i].shape}, Y Shape {fl_y_train[i].shape}\\n\")\n",
    "\n",
    "    # Write the testing data\n",
    "    f.write(f\"{datetime.datetime.now()} - X_test size: {X_test.shape}\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - y_test size: {y_test.shape}\\n\")\n",
    "    \n",
    "# close the file\n",
    "f.close()\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_OF_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_OF_ROUNDS),\n",
    "    strategy=strategy,\n",
    ")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Total time taken: \", end_time - start_time)\n",
    "\n",
    "print (f\"{Colours.YELLOW.value} SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n",
    "print (f\"Number of Clients = {NUM_OF_CLIENTS}{Colours.NORMAL.value}\\n\")\n",
    "\n",
    "# Output the same information to the Output/Run_details.txt file\n",
    "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n",
    "    f.write(f\"{datetime.datetime.now()} - SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - Total time taken: {end_time - start_time}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
