{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the available types of federated learning.\n",
    "\n",
    " - 'STRATIFIED': Stratified sampling of the data. The data is split into a number of shards, and each shard is assigned to a client. The data is split in a stratified manner, meaning that the distribution of the labels is approximately the same in each shard.\n",
    " - 'LEAVE_ONE_OUT' - Each client is assigned a shard of data, each shard is missing one of the attack labels. Other clients in the network are exposed to the attack label, but the specific client is not. This demonstrates the ability of federated learning to protect against unknown attacks.\n",
    " - 'ONE_CLASS' - Each client is assigned a shard of data, each shard contains only one of the attack labels.\n",
    " - 'HALF_BENIGN' - Half of the clients are exposed to Benign data only, the other half are exposed to all data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS SECTION NEEDS TO BE SET TO DETERMINE WHICH CONFIGURATION METHOD TO UTILISE\n",
    "\n",
    "SPLIT_AVAILABLE_METHODS = ['STRATIFIED','LEAVE_ONE_OUT', 'ONE_CLASS', 'HALF_BENIGN' ]\n",
    "METHOD = 'HALF_BENIGN'\n",
    "NUM_OF_STRATIFIED_CLIENTS = 10  # only applies to stratified method\n",
    "NUM_OF_ROUNDS = 10              # Number of FL rounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above test method in conjunction with the below classification selection will determine the number of clients.\n",
    "\n",
    "EG: \n",
    "`STRATIFIED` with:\n",
    " - `ALL TYPES` - Results in `NUM_OF_STRATIFIED_CLIENTS` clients. Each client will have a stratified sample of the data.\n",
    "\n",
    "`LEAVE_ONE_OUT` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 32 attack labels.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 6 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n",
    "\n",
    "`ONE_CLASS` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 1 attack label.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 1 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n",
    "\n",
    "`HALF_BENIGN` with:\n",
    " - `individual_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 33 malicious attack labels.\n",
    " - `group_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 7 malicious attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_classifier = False\n",
    "group_classifier = False\n",
    "binary_classifier = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the defines for the dataframe columns and the attack labels and their mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from includes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install flwr[simulation] torch torchvision matplotlib sklearn openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from flwr.common import Metrics\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flwr 1.4.0\n",
      "numpy 1.24.2\n",
      "torch 1.13.1\n",
      "Training on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRECTORY = '../datasets/CICIoT2023/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either read the training pickle file if it exists, or process the dataset from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists, loading data...\n",
      "Training data loaded from pickle file.\n",
      "Training data size: (1817320, 47)\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the file 'training_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
    "if os.path.isfile('training_data.pkl'):\n",
    "    print(\"File exists, loading data...\")\n",
    "    train_df = pd.read_pickle('training_data.pkl')\n",
    "    print(\"Training data loaded from pickle file.\")\n",
    "\n",
    "else:\n",
    "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "    df_sets.sort()\n",
    "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
    "\n",
    "    # Print the number of files in each set\n",
    "    print('Training sets: {}'.format(len(training_sets)))\n",
    "    print('Test sets: {}'.format(len(test_sets)))\n",
    "\n",
    "    # ######################\n",
    "    # # HACK TEMP CODE\n",
    "    # ######################\n",
    "    # # Set training_sets to the last entry of training_sets\n",
    "    # training_sets = training_sets[-33:]\n",
    "    # print(f\"HACK TO REPLICATE ORIGINAL AUTHORS CODE WITH ONE FILE TRAIN - {training_sets}\")\n",
    "    # #####################\n",
    "    # # HACK END TEMP CODE\n",
    "    # ######################\n",
    "\n",
    "    # Concatenate all training sets into one dataframe\n",
    "    dfs = []\n",
    "    print(\"Reading training data...\")\n",
    "    for train_set in tqdm(training_sets):\n",
    "        df_new = pd.read_csv(DATASET_DIRECTORY + train_set)\n",
    "        dfs.append(df_new)\n",
    "    train_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Complete training data set size\n",
    "    print(\"Complete training data size: {}\".format(train_df.shape))\n",
    "\n",
    "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
    "    train_df['label'] = train_df['label'].map(dict_34_classes)\n",
    "\n",
    "    # The training data is the 80% of the CSV files in the dataset. The test data is the remaining 20%.\n",
    "    # The Ray Federated learning mechanism cannot cope with all of the 80% training data, so we will split\n",
    "    # the training data using test_train_split. The test data will be ignored as we will use all the data \n",
    "    # from the train_sets files as our training data to keep parity with the original authors code.\n",
    "    # \n",
    "    # By using a subset of the training data split this way, we can have a randomised selection of data\n",
    "    # from all the training CSV files, stratified by the attack types.\n",
    "    \n",
    "    # Percentage of original training data to use.\n",
    "    TRAIN_SIZE = 0.05\n",
    "    \n",
    "    print(f\"Splitting the data into {TRAIN_SIZE*100}%\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_df[X_columns], train_df[y_column], test_size= (1 - TRAIN_SIZE), random_state=42, stratify=train_df[y_column])\n",
    "\n",
    "    # Recombine X_train, and y_train into a dataframe\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # Clean up unused variables\n",
    "\n",
    "    del X_train, y_train, X_test, y_test\n",
    "    \n",
    "    # Save the output to a pickle file\n",
    "    print(\"Writing training data to pickle file...\")\n",
    "    train_df.to_pickle('training_data.pkl')\n",
    "\n",
    "print(\"Training data size: {}\".format(train_df.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of attacks in train_df:\n",
      "6     280286\n",
      "4     210793\n",
      "5     175073\n",
      "2     159342\n",
      "3     157918\n",
      "1     157458\n",
      "7     140127\n",
      "13    129201\n",
      "15    103974\n",
      "14     78948\n",
      "0      42744\n",
      "17     38588\n",
      "19     34660\n",
      "18     29274\n",
      "10     17588\n",
      "26     11980\n",
      "9      11191\n",
      "8      11119\n",
      "25      6955\n",
      "24      5231\n",
      "21      3828\n",
      "22      3193\n",
      "16      2805\n",
      "23      1458\n",
      "12      1117\n",
      "11       907\n",
      "33       504\n",
      "27       231\n",
      "32       213\n",
      "31       204\n",
      "29       148\n",
      "28       125\n",
      "20        87\n",
      "30        50\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# show the unique values counts in the label column for train_df\n",
    "print(\"Counts of attacks in train_df:\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>Header_Length</th>\n",
       "      <th>Protocol Type</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Srate</th>\n",
       "      <th>Drate</th>\n",
       "      <th>fin_flag_number</th>\n",
       "      <th>syn_flag_number</th>\n",
       "      <th>rst_flag_number</th>\n",
       "      <th>...</th>\n",
       "      <th>Std</th>\n",
       "      <th>Tot size</th>\n",
       "      <th>IAT</th>\n",
       "      <th>Number</th>\n",
       "      <th>Magnitue</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Covariance</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Weight</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15361719</th>\n",
       "      <td>0.049451</td>\n",
       "      <td>18424.23</td>\n",
       "      <td>16.89</td>\n",
       "      <td>63.89</td>\n",
       "      <td>8773.276534</td>\n",
       "      <td>8773.276534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.748937</td>\n",
       "      <td>56.23</td>\n",
       "      <td>8.310203e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.057355</td>\n",
       "      <td>3.934737</td>\n",
       "      <td>387.053850</td>\n",
       "      <td>0.02</td>\n",
       "      <td>141.55</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13960994</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>3.162258</td>\n",
       "      <td>3.162258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>8.348236e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.165151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30358725</th>\n",
       "      <td>0.047289</td>\n",
       "      <td>108.46</td>\n",
       "      <td>6.11</td>\n",
       "      <td>69.00</td>\n",
       "      <td>1.180934</td>\n",
       "      <td>1.180934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.804392</td>\n",
       "      <td>58.96</td>\n",
       "      <td>8.294673e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.969024</td>\n",
       "      <td>19.544105</td>\n",
       "      <td>1184.679387</td>\n",
       "      <td>0.18</td>\n",
       "      <td>141.55</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374573</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>63.36</td>\n",
       "      <td>36.746530</td>\n",
       "      <td>36.746530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501296</td>\n",
       "      <td>42.18</td>\n",
       "      <td>8.314974e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.180184</td>\n",
       "      <td>0.710302</td>\n",
       "      <td>2.315749</td>\n",
       "      <td>0.11</td>\n",
       "      <td>141.55</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33149322</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>4.458184</td>\n",
       "      <td>4.458184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>8.315026e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.165151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26352975</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.46</td>\n",
       "      <td>5.94</td>\n",
       "      <td>63.36</td>\n",
       "      <td>24.662729</td>\n",
       "      <td>24.662729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327419</td>\n",
       "      <td>54.06</td>\n",
       "      <td>8.333105e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.404376</td>\n",
       "      <td>0.463495</td>\n",
       "      <td>0.631418</td>\n",
       "      <td>0.18</td>\n",
       "      <td>141.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21622577</th>\n",
       "      <td>4.942958</td>\n",
       "      <td>3631547.56</td>\n",
       "      <td>17.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>1338.337713</td>\n",
       "      <td>1338.337713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>554.00</td>\n",
       "      <td>8.370723e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>33.286634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31273160</th>\n",
       "      <td>0.186219</td>\n",
       "      <td>40025.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>4298.059589</td>\n",
       "      <td>4298.059589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00</td>\n",
       "      <td>8.301589e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27015198</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.39</td>\n",
       "      <td>6.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>18.222303</td>\n",
       "      <td>18.222303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.944377</td>\n",
       "      <td>54.39</td>\n",
       "      <td>8.306414e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.457111</td>\n",
       "      <td>2.752717</td>\n",
       "      <td>23.297396</td>\n",
       "      <td>0.17</td>\n",
       "      <td>141.55</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17580414</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>8.315071e+07</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.165151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.55</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1817320 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          flow_duration  Header_Length  Protocol Type  Duration         Rate  \\\n",
       "15361719       0.049451       18424.23          16.89     63.89  8773.276534   \n",
       "13960994       0.000000           0.00           1.00     64.00     3.162258   \n",
       "30358725       0.047289         108.46           6.11     69.00     1.180934   \n",
       "3374573        0.000000           0.00           0.99     63.36    36.746530   \n",
       "33149322       0.000000           0.00           1.00     64.00     4.458184   \n",
       "...                 ...            ...            ...       ...          ...   \n",
       "26352975       0.000000          53.46           5.94     63.36    24.662729   \n",
       "21622577       4.942958     3631547.56          17.00     64.00  1338.337713   \n",
       "31273160       0.186219       40025.00          17.00     64.00  4298.059589   \n",
       "27015198       0.000000          54.39           6.00     64.00    18.222303   \n",
       "17580414       0.000000           0.00           1.00     64.00     0.000000   \n",
       "\n",
       "                Srate  Drate  fin_flag_number  syn_flag_number  \\\n",
       "15361719  8773.276534    0.0              0.0              0.0   \n",
       "13960994     3.162258    0.0              0.0              0.0   \n",
       "30358725     1.180934    0.0              0.0              0.0   \n",
       "3374573     36.746530    0.0              0.0              0.0   \n",
       "33149322     4.458184    0.0              0.0              0.0   \n",
       "...               ...    ...              ...              ...   \n",
       "26352975    24.662729    0.0              0.0              0.0   \n",
       "21622577  1338.337713    0.0              0.0              0.0   \n",
       "31273160  4298.059589    0.0              0.0              0.0   \n",
       "27015198    18.222303    0.0              0.0              0.0   \n",
       "17580414     0.000000    0.0              0.0              0.0   \n",
       "\n",
       "          rst_flag_number  ...        Std  Tot size           IAT  Number  \\\n",
       "15361719              0.0  ...   2.748937     56.23  8.310203e+07     9.5   \n",
       "13960994              0.0  ...   0.000000     42.00  8.348236e+07     9.5   \n",
       "30358725              0.0  ...  13.804392     58.96  8.294673e+07     9.5   \n",
       "3374573               0.0  ...   0.501296     42.18  8.314974e+07     9.5   \n",
       "33149322              0.0  ...   0.000000     42.00  8.315026e+07     9.5   \n",
       "...                   ...  ...        ...       ...           ...     ...   \n",
       "26352975              0.0  ...   0.327419     54.06  8.333105e+07     9.5   \n",
       "21622577              0.0  ...   0.000000    554.00  8.370723e+07     9.5   \n",
       "31273160              0.0  ...   0.000000     50.00  8.301589e+07     9.5   \n",
       "27015198              0.0  ...   1.944377     54.39  8.306414e+07     9.5   \n",
       "17580414              0.0  ...   0.000000     42.00  8.315071e+07     9.5   \n",
       "\n",
       "           Magnitue     Radius   Covariance  Variance  Weight  label  \n",
       "15361719  10.057355   3.934737   387.053850      0.02  141.55      4  \n",
       "13960994   9.165151   0.000000     0.000000      0.00  141.55      6  \n",
       "30358725  10.969024  19.544105  1184.679387      0.18  141.55     15  \n",
       "3374573    9.180184   0.710302     2.315749      0.11  141.55      6  \n",
       "33149322   9.165151   0.000000     0.000000      0.00  141.55      6  \n",
       "...             ...        ...          ...       ...     ...    ...  \n",
       "26352975  10.404376   0.463495     0.631418      0.18  141.55      2  \n",
       "21622577  33.286634   0.000000     0.000000      0.00  141.55     19  \n",
       "31273160  10.000000   0.000000     0.000000      0.00  141.55     13  \n",
       "27015198  10.457111   2.752717    23.297396      0.17  141.55      5  \n",
       "17580414   9.165151   0.000000     0.000000      0.00  141.55      6  \n",
       "\n",
       "[1817320 rows x 47 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Data\n",
    "Concat the test data into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File testing_data.pkl exists, loading data...\n",
      "Test data loaded from pickle file.\n",
      "Testing data size: (10340161, 47)\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the file 'test_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
    "testing_data_pickle_file = 'testing_data.pkl'\n",
    "\n",
    "if os.path.isfile(testing_data_pickle_file):\n",
    "    print(f\"File {testing_data_pickle_file} exists, loading data...\")\n",
    "    test_df = pd.read_pickle(testing_data_pickle_file)\n",
    "    print(\"Test data loaded from pickle file.\")\n",
    "\n",
    "else:\n",
    "    print(f\"File {testing_data_pickle_file} does not exist, constructing data...\")\n",
    "\n",
    "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "    df_sets.sort()\n",
    "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
    "\n",
    "    ############################################\n",
    "    ############################################\n",
    "    # HACK - Make things quicker for now\n",
    "    ############################################\n",
    "    ############################################\n",
    "\n",
    "    # test_sets = df_sets[int(len(df_sets)*.95):]\n",
    "    \n",
    "    # # Set training_sets to the last entry of training_sets\n",
    "    # test_sets = test_sets[-2:]\n",
    "    \n",
    "    ############################################\n",
    "    ############################################\n",
    "    # END HACK \n",
    "    ############################################\n",
    "    ############################################\n",
    "\n",
    "    # Print the number of files in each set\n",
    "    print('Test sets: {}'.format(len(test_sets)))\n",
    "    \n",
    "    # Concatenate all testing sets into one dataframe\n",
    "    dfs = []\n",
    "    print(\"Reading test data...\")\n",
    "    for test_set in tqdm(test_sets):\n",
    "        df_new = pd.read_csv(DATASET_DIRECTORY + test_set)\n",
    "        dfs.append(df_new)\n",
    "    test_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
    "    test_df['label'] = test_df['label'].map(dict_34_classes)\n",
    "\n",
    "    # Save the output to a pickle file\n",
    "    print(f\"Writing test data to pickle file {testing_data_pickle_file}...\")\n",
    "    test_df.to_pickle(testing_data_pickle_file)\n",
    "\n",
    "print(\"Testing data size: {}\".format(test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train_df: 1817320\n",
      "Number of rows in test_df: 10340161\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in train_df: {}\".format(len(train_df)))\n",
    "print(\"Number of rows in test_df: {}\".format(len(test_df)))\n",
    "\n",
    "train_size = len(train_df)\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Scale the test and train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the training data input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df[X_columns] = scaler.fit_transform(train_df[X_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the testing data input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[X_columns] = scaler.fit_transform(test_df[X_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define the classification problem - (2 classes, 8 classes or 34 classes)\n",
    "Change the following cell to select the classification type\n",
    "\n",
    "If the METHOD == STRATIFIED, then we can use any classifier\n",
    "If the METHOD == ATTACK_GROUP then we must use Group Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary 2 Class Classifier... - Adjusting labels in test and train dataframes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_size_map = {2: \"Binary\", 8: \"Group\", 34: \"Individual\"}\n",
    "\n",
    "if group_classifier:\n",
    "    print(\"Group 8 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
    "    # Map y column to the dict_7_classes values\n",
    "    test_df['label'] = test_df['label'].map(dict_8_classes)\n",
    "    train_df['label'] = train_df['label'].map(dict_8_classes)\n",
    "    class_size = \"8\"      \n",
    "    \n",
    "elif binary_classifier:\n",
    "    print(\"Binary 2 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
    "    # Map y column to the dict_2_classes values\n",
    "    test_df['label'] = test_df['label'].map(dict_2_classes)\n",
    "    train_df['label'] = train_df['label'].map(dict_2_classes)\n",
    "    class_size = \"2\"\n",
    "\n",
    "else:\n",
    "    print (\"Individual 34 Class classifier... - No adjustments to labels in test and train dataframes\")\n",
    "    class_size = \"34\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Split the Training Data into partitions for the Federated Learning clients depending on the test required\n",
    "As a reminder:\n",
    "\n",
    "`STRATIFIED` with:\n",
    " - `ALL TYPES` - Results in `NUM_OF_STRATIFIED_CLIENTS` clients. Each client will have a stratified sample of the data.\n",
    "\n",
    "`LEAVE_ONE_OUT` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 32 attack labels.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 6 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n",
    "\n",
    "`ONE_CLASS` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 1 attack label.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 1 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n",
    "\n",
    "`HALF_BENIGN` with:\n",
    " - `individual_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 33 malicious attack labels.\n",
    " - `group_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 7 malicious attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mHALF_BENIGN_ONLY METHOD\u001b[0m with 2 class classifier\n",
      "Benign only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26652\\2411292254.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes\n",
      "Benign only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26652\\2411292254.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes\n",
      "Benign only\n",
      "All Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26652\\2411292254.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign only\n",
      "All Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26652\\2411292254.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign only\n",
      "All Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26652\\2411292254.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign only\n",
      "All Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26652\\2411292254.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign only\n",
      "All Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26652\\2411292254.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign only\n",
      "All Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26652\\2411292254.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign only\n",
      "All Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26652\\2411292254.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign only\n",
      "All Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon\\AppData\\Local\\Temp\\ipykernel_26652\\2411292254.py:99: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define fl_X_train and fl_y_train\n",
    "fl_X_train = []\n",
    "fl_y_train = []\n",
    "\n",
    "client_df = pd.DataFrame()\n",
    "\n",
    "if METHOD == 'STRATIFIED':\n",
    "    print(f\"{Colours.YELLOW.value}STRATIFIED METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "    # We are going to split the training data into 'NUM_OF_STRATIFIED_CLIENTS' smaller groups using StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=NUM_OF_STRATIFIED_CLIENTS, shuffle=True, random_state=42)\n",
    "    for train_index, test_index in skf.split(train_df[X_columns], train_df[y_column]):\n",
    "        fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "        fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "\n",
    "elif METHOD == 'LEAVE_ONE_OUT':\n",
    "    print(f\"{Colours.YELLOW.value}LEAVE_ONE_OUT METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "\n",
    "    if individual_classifier or group_classifier:\n",
    "        # Set the number of splits required to the number of classes - 1\n",
    "        num_splits = int(class_size) - 1\n",
    "    else:\n",
    "        # For binary classifier, set the number of splits to 10\n",
    "        num_splits = 10\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # When creating the clients, we will remove one attack class from the training data\n",
    "    # For the binary classifier, evey other client will have the benign class removed\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
    "        if binary_classifier:\n",
    "            print(f\"i: {i} = i % 2 = {i % 2}\")\n",
    "            if i % 2 == 0:\n",
    "                print(\"Benign only\")\n",
    "                # Create a new dataframe for the client data with only benign traffic\n",
    "                client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != 1]], ignore_index=True)\n",
    "                fl_X_train.append(client_df[X_columns])\n",
    "                fl_y_train.append(client_df[y_column])\n",
    "            else:\n",
    "                print(\"Both\")\n",
    "                # Create a new dataframe for the client data\n",
    "                fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "                fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "        else:\n",
    "            # Create a new dataframe for the client data\n",
    "            client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != i+1]], ignore_index=True)\n",
    "            fl_X_train.append(client_df[X_columns])\n",
    "            fl_y_train.append(client_df[y_column])\n",
    "\n",
    "elif METHOD == 'ONE_CLASS':\n",
    "    print(f\"{Colours.YELLOW.value}ONE_CLASS METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "    # Each client only has one attack class in their training data along with the Benign data\n",
    "    \n",
    "    if individual_classifier or group_classifier:\n",
    "        # Set the number of splits required to the number of classes - 1\n",
    "        num_splits = int(class_size) - 1\n",
    "    else:\n",
    "        # For binary classifier, set the number of splits to 10\n",
    "        num_splits = 10\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # When creating the clients, we will only add the benign data and the attack class for that client\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
    "        if binary_classifier:\n",
    "            print(f\"i: {i} = i % 2 = {i % 2}\")\n",
    "            if i % 2 == 0:\n",
    "                print(\"Benign only\")\n",
    "                # Create a new dataframe for the client data with only benign traffic\n",
    "                client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != 1]], ignore_index=True)\n",
    "                fl_X_train.append(client_df[X_columns])\n",
    "                fl_y_train.append(client_df[y_column])\n",
    "            else:\n",
    "                print(\"Both\")\n",
    "                # Create a new dataframe for the client data\n",
    "                fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "                fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "        else:\n",
    "            # Create a new dataframe for the client data\n",
    "            client_df = pd.concat([train_df.iloc[test_index][(train_df[y_column] == 0) | (train_df[y_column] == i+1)]], ignore_index=True)\n",
    "            fl_X_train.append(client_df[X_columns])\n",
    "            fl_y_train.append(client_df[y_column])\n",
    "\n",
    "elif METHOD == 'HALF_BENIGN':\n",
    "    print(f\"{Colours.YELLOW.value}HALF_BENIGN METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "\n",
    "    num_splits = 10\n",
    "\n",
    "    # Split into 10 client data\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    # For i % 2 == 0, add only benign data\n",
    "    # For i % 2 == 1, add all data\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
    "        if i % 2 == 0:\n",
    "            print(\"Benign only\")\n",
    "            # Create a new dataframe for the client data with only benign traffic\n",
    "            client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n",
    "            fl_X_train.append(client_df[X_columns])\n",
    "            fl_y_train.append(client_df[y_column])\n",
    "        else:\n",
    "            print(\"All Classes\")\n",
    "            fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "            fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "else:\n",
    "    print(f\"{Colours.RED.value}ERROR: Method {METHOD} not recognised{Colours.NORMAL.value}\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fl_X_train[0].shape: (2137, 46)\n",
      "fl_y_train[0].value_counts():\n",
      "0    2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[0].unique(): [0]\n",
      "\n",
      "fl_X_train[1].shape: (90866, 46)\n",
      "fl_y_train[1].value_counts():\n",
      "1    88729\n",
      "0     2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[1].unique(): [1 0]\n",
      "\n",
      "fl_X_train[2].shape: (2137, 46)\n",
      "fl_y_train[2].value_counts():\n",
      "0    2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[2].unique(): [0]\n",
      "\n",
      "fl_X_train[3].shape: (90866, 46)\n",
      "fl_y_train[3].value_counts():\n",
      "1    88729\n",
      "0     2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[3].unique(): [1 0]\n",
      "\n",
      "fl_X_train[4].shape: (2137, 46)\n",
      "fl_y_train[4].value_counts():\n",
      "0    2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[4].unique(): [0]\n",
      "\n",
      "fl_X_train[5].shape: (90866, 46)\n",
      "fl_y_train[5].value_counts():\n",
      "1    88729\n",
      "0     2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[5].unique(): [1 0]\n",
      "\n",
      "fl_X_train[6].shape: (2137, 46)\n",
      "fl_y_train[6].value_counts():\n",
      "0    2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[6].unique(): [0]\n",
      "\n",
      "fl_X_train[7].shape: (90866, 46)\n",
      "fl_y_train[7].value_counts():\n",
      "1    88729\n",
      "0     2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[7].unique(): [1 0]\n",
      "\n",
      "fl_X_train[8].shape: (2137, 46)\n",
      "fl_y_train[8].value_counts():\n",
      "0    2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[8].unique(): [0]\n",
      "\n",
      "fl_X_train[9].shape: (90866, 46)\n",
      "fl_y_train[9].value_counts():\n",
      "1    88729\n",
      "0     2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[9].unique(): [1 0]\n",
      "\n",
      "fl_X_train[10].shape: (2137, 46)\n",
      "fl_y_train[10].value_counts():\n",
      "0    2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[10].unique(): [0]\n",
      "\n",
      "fl_X_train[11].shape: (90866, 46)\n",
      "fl_y_train[11].value_counts():\n",
      "1    88729\n",
      "0     2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[11].unique(): [1 0]\n",
      "\n",
      "fl_X_train[12].shape: (2137, 46)\n",
      "fl_y_train[12].value_counts():\n",
      "0    2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[12].unique(): [0]\n",
      "\n",
      "fl_X_train[13].shape: (90866, 46)\n",
      "fl_y_train[13].value_counts():\n",
      "1    88729\n",
      "0     2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[13].unique(): [1 0]\n",
      "\n",
      "fl_X_train[14].shape: (2137, 46)\n",
      "fl_y_train[14].value_counts():\n",
      "0    2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[14].unique(): [0]\n",
      "\n",
      "fl_X_train[15].shape: (90866, 46)\n",
      "fl_y_train[15].value_counts():\n",
      "1    88729\n",
      "0     2137\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[15].unique(): [1 0]\n",
      "\n",
      "fl_X_train[16].shape: (2138, 46)\n",
      "fl_y_train[16].value_counts():\n",
      "0    2138\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[16].unique(): [0]\n",
      "\n",
      "fl_X_train[17].shape: (90866, 46)\n",
      "fl_y_train[17].value_counts():\n",
      "1    88728\n",
      "0     2138\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[17].unique(): [1 0]\n",
      "\n",
      "fl_X_train[18].shape: (2138, 46)\n",
      "fl_y_train[18].value_counts():\n",
      "0    2138\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[18].unique(): [0]\n",
      "\n",
      "fl_X_train[19].shape: (90866, 46)\n",
      "fl_y_train[19].value_counts():\n",
      "1    88728\n",
      "0     2138\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[19].unique(): [1 0]\n",
      "\n",
      "fl_X_train[0].equals(fl_X_train[1]): False\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_CLIENTS = len(fl_X_train)\n",
    "\n",
    "for i in range(len(fl_X_train)):\n",
    "    # Show the unique values in the y column\n",
    "    (f\"Client ID: {i}\")\n",
    "    print(f\"fl_X_train[{i}].shape: {fl_X_train[i].shape}\")  \n",
    "    print(f\"fl_y_train[{i}].value_counts():\\n{fl_y_train[i].value_counts()}\")\n",
    "    print(f\"fl_y_train[{i}].unique(): {fl_y_train[i].unique()}\\n\")\n",
    "\n",
    "# Check that fl_X_train[0] and fl_X_train[1] contain different data\n",
    "print(f\"fl_X_train[0].equals(fl_X_train[1]): {fl_X_train[0].equals(fl_X_train[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare an output directory where we can store the results of the federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an \"Output\" directory if it doesnt exist already\n",
    "if not os.path.exists(\"Output\"):\n",
    "    os.makedirs(\"Output\")\n",
    "\n",
    "sub_dir_name = f\"train_size-{train_size}_test_size-{test_size}\"\n",
    "\n",
    "# if sub_dir_name does not exist, create it\n",
    "if not os.path.exists(f\"Output/{sub_dir_name}\"):\n",
    "    os.makedirs(f\"Output/{sub_dir_name}\")\n",
    "\n",
    "test_directory_name = f\"{METHOD}_Classifier-{class_size}_Clients-{NUM_OF_CLIENTS}\"\n",
    "\n",
    "# Create an \"Output/{METHOD}-{NUM_OF_CLIENTS}-{NUM_OF_ROUNDS}\" directory if it doesnt exist already\n",
    "if not os.path.exists(f\"Output/{sub_dir_name}/{test_directory_name}\"):\n",
    "    os.makedirs(f\"Output/{sub_dir_name}/{test_directory_name}\")\n",
    "\n",
    "# Ensure the directory is empty\n",
    "for file in os.listdir(f\"Output/{sub_dir_name}/{test_directory_name}\"):\n",
    "    file_path = os.path.join(f\"Output/{sub_dir_name}/{test_directory_name}\", file)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.unlink(file_path)\n",
    "\n",
    "# Original training size is the sum of all the fl_X_train sizes\n",
    "original_training_size = 0\n",
    "for i in range(len(fl_X_train)):\n",
    "    original_training_size += fl_X_train[i].shape[0]\n",
    "\n",
    "# Write this same info to the output directory/Class Split Info.txt\n",
    "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Class Split Info.txt\", \"w\") as f:\n",
    "    for i in range(len(fl_X_train)):\n",
    "        f.write(f\"Client ID: {i}\\n\")\n",
    "        f.write(f\"fl_X_train.shape: {fl_X_train[i].shape}\\n\")\n",
    "        f.write(f\"Training data used {original_training_size}\")\n",
    "        f.write(f\"fl_y_train.value_counts():\\n{fl_y_train[i].value_counts()}\\n\")\n",
    "        f.write(f\"fl_y_train.unique(): {fl_y_train[i].unique()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the testing daya to X_test and y_test ndarrays\n",
    "X_test = test_df[X_columns].to_numpy()\n",
    "y_test = test_df[y_column].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_classes = len(train_df[y_column].unique())\n",
    "\n",
    "train_df_shape = train_df.shape\n",
    "test_df_shape = test_df.shape\n",
    "\n",
    "# We are now done with the train_df and test_df dataframes, so we can delete them to free up memory\n",
    "del train_df\n",
    "del test_df\n",
    "del client_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CLIENTS: 20\n",
      "NUM_ROUNDS: 10\n",
      "\n",
      "Original training size: 930032\n",
      "Checking training data split groups\n",
      "0 : X Shape (2137, 46) Y Shape (2137,)\n",
      "1 : X Shape (90866, 46) Y Shape (90866,)\n",
      "2 : X Shape (2137, 46) Y Shape (2137,)\n",
      "3 : X Shape (90866, 46) Y Shape (90866,)\n",
      "4 : X Shape (2137, 46) Y Shape (2137,)\n",
      "5 : X Shape (90866, 46) Y Shape (90866,)\n",
      "6 : X Shape (2137, 46) Y Shape (2137,)\n",
      "7 : X Shape (90866, 46) Y Shape (90866,)\n",
      "8 : X Shape (2137, 46) Y Shape (2137,)\n",
      "9 : X Shape (90866, 46) Y Shape (90866,)\n",
      "10 : X Shape (2137, 46) Y Shape (2137,)\n",
      "11 : X Shape (90866, 46) Y Shape (90866,)\n",
      "12 : X Shape (2137, 46) Y Shape (2137,)\n",
      "13 : X Shape (90866, 46) Y Shape (90866,)\n",
      "14 : X Shape (2137, 46) Y Shape (2137,)\n",
      "15 : X Shape (90866, 46) Y Shape (90866,)\n",
      "16 : X Shape (2138, 46) Y Shape (2138,)\n",
      "17 : X Shape (90866, 46) Y Shape (90866,)\n",
      "18 : X Shape (2138, 46) Y Shape (2138,)\n",
      "19 : X Shape (90866, 46) Y Shape (90866,)\n",
      "\n",
      "Checking testing data\n",
      "X_test size: (10340161, 46)\n",
      "y_test size: (10340161,)\n",
      "\n",
      "Deploy Simulation\n"
     ]
    }
   ],
   "source": [
    "print(\"NUM_CLIENTS:\", NUM_OF_CLIENTS)\n",
    "\n",
    "print(\"NUM_ROUNDS:\", NUM_OF_ROUNDS)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Original training size: {}\".format(original_training_size))\n",
    "\n",
    "\n",
    "print(\"Checking training data split groups\")\n",
    "for i in range(len(fl_X_train)):\n",
    "    print(i, \":\", \"X Shape\", fl_X_train[i].shape, \"Y Shape\", fl_y_train[i].shape)\n",
    "\n",
    "\n",
    "# Print the sizes of X_test and y_test\n",
    "print(\"\\nChecking testing data\")\n",
    "print(\"X_test size: {}\".format(X_test.shape))\n",
    "print(\"y_test size: {}\".format(y_test.shape))\n",
    "\n",
    "print(\"\\nDeploy Simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Federated Learning\n",
    "## Import the libraries and print the versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Client and Server code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn 1.2.0.\n",
      "flwr 1.4.0\n",
      "numpy 1.24.2\n",
      "tf 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print('scikit-learn {}.'.format(sklearn.__version__))\n",
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"tf\", tf.__version__)\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "import datetime\n",
    "\n",
    "client_evaluations = []\n",
    "\n",
    "class NumpyFlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, model, train_data, train_labels):\n",
    "        self.model = model\n",
    "        self.cid = cid\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Training...\")\n",
    "        self.model.fit(self.train_data, self.train_labels, epochs=5, batch_size=32)\n",
    "        print (\"Client \", self.cid, \"Training complete...\")\n",
    "        return self.model.get_weights(), len(self.train_data), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Evaluating...\")\n",
    "        loss, accuracy = self.model.evaluate(self.train_data, self.train_labels, batch_size=32)\n",
    "        print(f\"{Colours.YELLOW.value}Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}{Colours.NORMAL.value}\")\n",
    "\n",
    "        # Write the same message to the \"Output/{cid}_Evaluation.txt\" file\n",
    "        with open(f\"Output/{sub_dir_name}/{test_directory_name}/{self.cid}_Evaluation.txt\", \"a\") as f:\n",
    "            f.write(f\"{datetime.datetime.now()} - Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}\\n\")\n",
    "\n",
    "            # Close the file\n",
    "            f.close()\n",
    "\n",
    "        return loss, len(self.train_data), {\"accuracy\": accuracy}\n",
    "    \n",
    "    def predict(self, incoming):\n",
    "        prediction = np.argmax( self.model.predict(incoming) ,axis=1)\n",
    "        return prediction\n",
    "\n",
    "def client_fn(cid: str) -> NumpyFlowerClient:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    #model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n",
    "    #model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    print (\"Client ID:\", cid)\n",
    "\n",
    "    model = Sequential([\n",
    "      #Flatten(input_shape=(79,1)),\n",
    "      Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "      Dense(50, activation='relu'),  \n",
    "      Dense(25, activation='relu'),  \n",
    "      Dense(num_unique_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "   \n",
    "    partition_id = int(cid)\n",
    "    X_train_c = fl_X_train[partition_id]\n",
    "    y_train_c = fl_y_train[partition_id]\n",
    "\n",
    "    # Create a  single Flower client representing a single organization\n",
    "    return NumpyFlowerClient(cid, model, X_train_c, y_train_c)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "eval_count = 0\n",
    "\n",
    "def get_evaluate_fn(server_model):\n",
    "    global eval_count\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "    # The `evaluate` function will be called after every round\n",
    "    \n",
    "    \n",
    "    def evaluate(server_round, parameters, config):\n",
    "        global eval_count\n",
    "        \n",
    "        # Update model with the latest parameters\n",
    "        server_model.set_weights(parameters)\n",
    "        print (f\"Server Evaluating... Evaluation Count:{eval_count}\")\n",
    "        loss, accuracy = server_model.evaluate(X_test, y_test)\n",
    "        \n",
    "        y_pred = server_model.predict(X_test)\n",
    "        print (\"Prediction: \", y_pred, y_pred.shape)\n",
    "        #cmatrix = confusion_matrix(y_test, np.rint(y_pred))\n",
    "        #print (\"confusion_matrix:\", cmatrix, cmatrix.shape)\n",
    "                        \n",
    "        print(f\"{Colours.YELLOW.value}Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}{Colours.NORMAL.value}\")\n",
    "\n",
    "        # Write the same message to the \"Output/Server_Evaluation.txt\" file\n",
    "        with open(f\"Output/{sub_dir_name}/{test_directory_name}/Server_Evaluation.txt\", \"a\") as f:\n",
    "            f.write(f\"{datetime.datetime.now()} - {server_round} : Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\\n\")\n",
    "\n",
    "            # Close the file\n",
    "            f.close()\n",
    "        \n",
    "        np.save(\"y_pred-\" + str(eval_count) + \".npy\", y_pred)\n",
    "        #np.save(\"cmatrix-\" + str(eval_count) + \".npy\", cmatrix)\n",
    "        eval_count = eval_count + 1\n",
    "        \n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "    return evaluate\n",
    "\n",
    "\n",
    "\n",
    "server_model = Sequential([\n",
    "    #Flatten(input_shape=(79,1)),\n",
    "    Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "    Dense(50, activation='relu'),  \n",
    "    Dense(25, activation='relu'),  \n",
    "    Dense(num_unique_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "server_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=2, #10,\n",
    "        min_evaluate_clients=2, #5,\n",
    "        min_available_clients=2, #10,\n",
    "        evaluate_fn=get_evaluate_fn(server_model),\n",
    "        #evaluate_metrics_aggregation_fn=weighted_average,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-07-15 13:20:18,762 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\n",
      "Deploy simulation... Method = HALF_BENIGN_ONLY - Binary (2) Classifier\n",
      "Number of Clients = 20\n",
      "\n",
      "Writing output to: train_size-1817320_test_size-10340161/HALF_BENIGN_ONLY_Classifier-2_Clients-20\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-15 13:20:27,685\tINFO worker.py:1636 -- Started a local Ray instance.\n",
      "INFO flwr 2023-07-15 13:20:32,121 | app.py:180 | Flower VCE: Ray initialized with resources: {'memory': 18687546164.0, 'object_store_memory': 9343773081.0, 'node:127.0.0.1': 1.0, 'GPU': 1.0, 'CPU': 24.0}\n",
      "INFO flwr 2023-07-15 13:20:32,122 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-07-15 13:20:32,122 | server.py:273 | Requesting initial parameters from one random client\n",
      "INFO flwr 2023-07-15 13:20:36,614 | server.py:277 | Received initial parameters from one random client\n",
      "INFO flwr 2023-07-15 13:20:36,614 | server.py:88 | Evaluating initial parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=29172)\u001b[0m Client ID: 8\n",
      "Server Evaluating... Evaluation Count:0\n",
      "162770/323131 [==============>...............] - ETA: 2:01 - loss: 0.6117 - accuracy: 0.7445"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:27\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\flwr\\simulation\\app.py:197\u001b[0m, in \u001b[0;36mstart_simulation\u001b[1;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised)\u001b[0m\n\u001b[0;32m    194\u001b[0m     initialized_server\u001b[39m.\u001b[39mclient_manager()\u001b[39m.\u001b[39mregister(client\u001b[39m=\u001b[39mclient_proxy)\n\u001b[0;32m    196\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m hist \u001b[39m=\u001b[39m _fl(\n\u001b[0;32m    198\u001b[0m     server\u001b[39m=\u001b[39;49minitialized_server,\n\u001b[0;32m    199\u001b[0m     config\u001b[39m=\u001b[39;49minitialized_config,\n\u001b[0;32m    200\u001b[0m )\n\u001b[0;32m    202\u001b[0m event(EventType\u001b[39m.\u001b[39mSTART_SIMULATION_LEAVE)\n\u001b[0;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m hist\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\flwr\\server\\app.py:217\u001b[0m, in \u001b[0;36m_fl\u001b[1;34m(server, config)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fl\u001b[39m(\n\u001b[0;32m    213\u001b[0m     server: Server,\n\u001b[0;32m    214\u001b[0m     config: ServerConfig,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m History:\n\u001b[0;32m    216\u001b[0m     \u001b[39m# Fit model\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m     hist \u001b[39m=\u001b[39m server\u001b[39m.\u001b[39;49mfit(num_rounds\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rounds, timeout\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mround_timeout)\n\u001b[0;32m    218\u001b[0m     log(INFO, \u001b[39m\"\u001b[39m\u001b[39mapp_fit: losses_distributed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(hist\u001b[39m.\u001b[39mlosses_distributed))\n\u001b[0;32m    219\u001b[0m     log(INFO, \u001b[39m\"\u001b[39m\u001b[39mapp_fit: metrics_distributed_fit \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(hist\u001b[39m.\u001b[39mmetrics_distributed_fit))\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\flwr\\server\\server.py:89\u001b[0m, in \u001b[0;36mServer.fit\u001b[1;34m(self, num_rounds, timeout)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_initial_parameters(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m     88\u001b[0m log(INFO, \u001b[39m\"\u001b[39m\u001b[39mEvaluating initial parameters\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mevaluate(\u001b[39m0\u001b[39;49m, parameters\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters)\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     log(\n\u001b[0;32m     92\u001b[0m         INFO,\n\u001b[0;32m     93\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minitial parameters (loss, other metrics): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m         res[\u001b[39m0\u001b[39m],\n\u001b[0;32m     95\u001b[0m         res[\u001b[39m1\u001b[39m],\n\u001b[0;32m     96\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\flwr\\server\\strategy\\fedavg.py:164\u001b[0m, in \u001b[0;36mFedAvg.evaluate\u001b[1;34m(self, server_round, parameters)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    163\u001b[0m parameters_ndarrays \u001b[39m=\u001b[39m parameters_to_ndarrays(parameters)\n\u001b[1;32m--> 164\u001b[0m eval_res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_fn(server_round, parameters_ndarrays, {})\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m eval_res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Jon\\Documents\\VSCode Projects\\CICIoT2023\\03-Federated Learning.ipynb Cell 44\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jon/Documents/VSCode%20Projects/CICIoT2023/03-Federated%20Learning.ipynb#X61sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m server_model\u001b[39m.\u001b[39mset_weights(parameters)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jon/Documents/VSCode%20Projects/CICIoT2023/03-Federated%20Learning.ipynb#X61sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mServer Evaluating... Evaluation Count:\u001b[39m\u001b[39m{\u001b[39;00meval_count\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Jon/Documents/VSCode%20Projects/CICIoT2023/03-Federated%20Learning.ipynb#X61sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m loss, accuracy \u001b[39m=\u001b[39m server_model\u001b[39m.\u001b[39;49mevaluate(X_test, y_test)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jon/Documents/VSCode%20Projects/CICIoT2023/03-Federated%20Learning.ipynb#X61sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m y_pred \u001b[39m=\u001b[39m server_model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jon/Documents/VSCode%20Projects/CICIoT2023/03-Federated%20Learning.ipynb#X61sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mPrediction: \u001b[39m\u001b[39m\"\u001b[39m, y_pred, y_pred\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\keras\\engine\\training.py:2035\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2033\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[0;32m   2034\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 2035\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2036\u001b[0m         \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   2037\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   2038\u001b[0m         ):\n\u001b[0;32m   2039\u001b[0m             callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\keras\\engine\\data_adapter.py:1371\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1370\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1371\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1372\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1373\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1374\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[0;32m   1376\u001b[0m )\n\u001b[0;32m   1378\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:639\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 639\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    640\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    641\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:727\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[39m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \n\u001b[0;32m    720\u001b[0m \u001b[39mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mRead\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 727\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_variable_op()\n\u001b[0;32m    728\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39midentity(value)\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:706\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    704\u001b[0m       result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    705\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m   result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    709\u001b[0m   \u001b[39m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    710\u001b[0m   \u001b[39m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m   tape\u001b[39m.\u001b[39mrecord_operation(\n\u001b[0;32m    712\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadVariableOp\u001b[39m\u001b[39m\"\u001b[39m, [result], [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle],\n\u001b[0;32m    713\u001b[0m       backward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    714\u001b[0m       forward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:696\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[39mif\u001b[39;00m no_copy \u001b[39mand\u001b[39;00m forward_compat\u001b[39m.\u001b[39mforward_compatible(\u001b[39m2022\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m    695\u001b[0m   gen_resource_variable_ops\u001b[39m.\u001b[39mdisable_copy_on_read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)\n\u001b[1;32m--> 696\u001b[0m result \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mread_variable_op(\n\u001b[0;32m    697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[0;32m    698\u001b[0m _maybe_set_handle_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, result)\n\u001b[0;32m    699\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:580\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    579\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 580\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    581\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mReadVariableOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, resource, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype)\n\u001b[0;32m    582\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    583\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print (f\"{Colours.YELLOW.value}\\nDeploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n",
    "print (f\"Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
    "print (f\"Writing output to: {sub_dir_name}/{test_directory_name}\\n{Colours.NORMAL.value}\")\n",
    "\n",
    "# Output the same information to the Output/Run_details.txt file\n",
    "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n",
    "    f.write(f\"{datetime.datetime.now()} - Deploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
    "\n",
    "    # Write Original train_df size\n",
    "    f.write(f\"{datetime.datetime.now()} - Original train_df size: {train_df_shape}\\n\")\n",
    "\n",
    "    # Write the training data split groups\n",
    "    for i in range(len(fl_X_train)):\n",
    "        f.write(f\"{datetime.datetime.now()} - {i}: X Shape {fl_X_train[i].shape}, Y Shape {fl_y_train[i].shape}\\n\")\n",
    "\n",
    "    # Write the testing data\n",
    "    f.write(f\"{datetime.datetime.now()} - X_test size: {X_test.shape}\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - y_test size: {y_test.shape}\\n\")\n",
    "    \n",
    "# close the file\n",
    "f.close()\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_OF_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_OF_ROUNDS),\n",
    "    strategy=strategy,\n",
    ")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Total time taken: \", end_time - start_time)\n",
    "\n",
    "print (f\"{Colours.YELLOW.value} SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n",
    "print (f\"Number of Clients = {NUM_OF_CLIENTS}{Colours.NORMAL.value}\\n\")\n",
    "\n",
    "# Output the same information to the Output/Run_details.txt file\n",
    "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n",
    "    f.write(f\"{datetime.datetime.now()} - SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - Total time taken: {end_time - start_time}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
