{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the available types of federated learning.\n",
    "\n",
    " - 'STRATIFIED': Stratified sampling of the data. The data is split into a number of shards, and each shard is assigned to a client. The data is split in a stratified manner, meaning that the distribution of the labels is approximately the same in each shard.\n",
    " - 'MISSING_1_ATTACK' - Each client is assigned a shard of data, each shard is missing one of the attack labels. Other clients in the network are exposed to the attack label, but the specific client is not. This demonstrates the ability of federated learning to protect against unknown attacks.\n",
    " - '1_ATTACK_ONLY' - Each client is assigned a shard of data, each shard contains only one of the attack labels.\n",
    " - 'HALF_BENIGN_ONLY' - Half of the clients are exposed to Benign data only, the other half are exposed to all data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS SECTION NEEDS TO BE SET TO DETERMINE WHICH CONFIGURATION METHOD TO UTILISE\n",
    "\n",
    "SPLIT_AVAILABLE_METHODS = ['STRATIFIED','MISSING_1_ATTACK', '1_ATTACK_ONLY', 'HALF_BENIGN_ONLY' ]\n",
    "METHOD = 'STRATIFIED'\n",
    "NUM_OF_STRATIFIED_CLIENTS = 10  # only applies to stratified method\n",
    "NUM_OF_ROUNDS = 10              # Number of FL rounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above test method in conjunction with the below classification selection will determine the number of clients.\n",
    "\n",
    "EG: \n",
    "`STRATIFIED` with:\n",
    " - `ALL TYPES` - Results in `NUM_OF_STRATIFIED_CLIENTS` clients. Each client will have a stratified sample of the data.\n",
    "\n",
    "`MISSING_1_ATTACK` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 32 attack labels.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 6 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n",
    "\n",
    "`1_ATTACK_ONLY` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 1 attack label.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 1 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n",
    "\n",
    "`HALF_BENIGN_ONLY` with:\n",
    " - `individual_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 33 malicious attack labels.\n",
    " - `group_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 7 malicious attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and Malicious attack labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_classifier = True\n",
    "group_classifier = False\n",
    "binary_classifier = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the defines for the dataframe columns and the attack labels and their mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from includes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install flwr[simulation] torch torchvision matplotlib sklearn openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from flwr.common import Metrics\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRECTORY = '../datasets/CICIoT2023/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either read the training pickle file if it exists, or process the dataset from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the file 'training_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
    "if os.path.isfile('training_data.pkl'):\n",
    "    print(\"File exists, loading data...\")\n",
    "    train_df = pd.read_pickle('training_data.pkl')\n",
    "    print(\"Training data loaded from pickle file.\")\n",
    "\n",
    "else:\n",
    "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "    df_sets.sort()\n",
    "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
    "\n",
    "    # Print the number of files in each set\n",
    "    print('Training sets: {}'.format(len(training_sets)))\n",
    "    print('Test sets: {}'.format(len(test_sets)))\n",
    "\n",
    "    ######################\n",
    "    # HACK TEMP CODE\n",
    "    ######################\n",
    "    # Set training_sets to the last entry of training_sets\n",
    "    training_sets = training_sets[-5:]\n",
    "    print(f\"HACK TO REPLICATE ORIGINAL AUTHORS CODE WITH ONE FILE TRAIN - {training_sets}\")\n",
    "    ######################\n",
    "    # HACK END TEMP CODE\n",
    "    ######################\n",
    "\n",
    "    # Concatenate all training sets into one dataframe\n",
    "    dfs = []\n",
    "    print(\"Reading training data...\")\n",
    "    for train_set in tqdm(training_sets):\n",
    "        df_new = pd.read_csv(DATASET_DIRECTORY + train_set)\n",
    "        dfs.append(df_new)\n",
    "    train_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
    "    train_df['label'] = train_df['label'].map(dict_34_classes)\n",
    "\n",
    "    # Save the output to a pickle file\n",
    "    print(\"Writing training data to pickle file...\")\n",
    "    train_df.to_pickle('training_data.pkl')\n",
    "\n",
    "print(\"Training data size: {}\".format(train_df.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Data\n",
    "Concat the test data into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the file 'test_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
    "testing_data_pickle_file = 'testing_data.pkl'\n",
    "\n",
    "if os.path.isfile(testing_data_pickle_file):\n",
    "    print(f\"File {testing_data_pickle_file} exists, loading data...\")\n",
    "    test_df = pd.read_pickle(testing_data_pickle_file)\n",
    "    print(\"Test data loaded from pickle file.\")\n",
    "\n",
    "else:\n",
    "    print(f\"File {testing_data_pickle_file} does not exist, constructing data...\")\n",
    "\n",
    "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "    df_sets.sort()\n",
    "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
    "\n",
    "    ############################################\n",
    "    ############################################\n",
    "    # HACK - Make things quicker for now\n",
    "    ############################################\n",
    "    ############################################\n",
    "\n",
    "    test_sets = df_sets[int(len(df_sets)*.95):]\n",
    "    \n",
    "    # Set training_sets to the last entry of training_sets\n",
    "    test_sets = test_sets[-2:]\n",
    "    \n",
    "    ############################################\n",
    "    ############################################\n",
    "    # END HACK \n",
    "    ############################################\n",
    "    ############################################\n",
    "\n",
    "    # Print the number of files in each set\n",
    "    print('Test sets: {}'.format(len(test_sets)))\n",
    "    \n",
    "    # Concatenate all testing sets into one dataframe\n",
    "    dfs = []\n",
    "    print(\"Reading test data...\")\n",
    "    for test_set in tqdm(test_sets):\n",
    "        df_new = pd.read_csv(DATASET_DIRECTORY + test_set)\n",
    "        dfs.append(df_new)\n",
    "    test_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
    "    test_df['label'] = test_df['label'].map(dict_34_classes)\n",
    "\n",
    "    # Save the output to a pickle file\n",
    "    print(f\"Writing test data to pickle file {testing_data_pickle_file}...\")\n",
    "    test_df.to_pickle(testing_data_pickle_file)\n",
    "\n",
    "print(\"Testing data size: {}\".format(test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Scale the test and train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the training data input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df[X_columns] = scaler.fit_transform(train_df[X_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the testing data input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[X_columns] = scaler.fit_transform(test_df[X_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define the classification problem - (2 classes, 8 classes or 34 classes)\n",
    "Change the following cell to select the classification type\n",
    "\n",
    "If the METHOD == STRATIFIED, then we can use any classifier\n",
    "If the METHOD == ATTACK_GROUP then we must use Group Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 34 Class classifier... - No adjustments to labels in test and train dataframes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_size_map = {2: \"Binary\", 8: \"Group\", 34: \"Individual\"}\n",
    "\n",
    "if group_classifier:\n",
    "    print(\"Group 8 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
    "    # Map y column to the dict_7_classes values\n",
    "    test_df['label'] = test_df['label'].map(dict_8_classes)\n",
    "    train_df['label'] = train_df['label'].map(dict_8_classes)\n",
    "    class_size = \"8\"      \n",
    "    \n",
    "elif binary_classifier:\n",
    "    print(\"Binary 2 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
    "    # Map y column to the dict_2_classes values\n",
    "    test_df['label'] = test_df['label'].map(dict_2_classes)\n",
    "    train_df['label'] = train_df['label'].map(dict_2_classes)\n",
    "    class_size = \"2\"\n",
    "\n",
    "else:\n",
    "    print (\"Individual 34 Class classifier... - No adjustments to labels in test and train dataframes\")\n",
    "    class_size = \"34\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Split the Training Data into partitions for the Federated Learning clients depending on the test required\n",
    "As a reminder:\n",
    "\n",
    "`STRATIFIED` with:\n",
    " - `ALL TYPES` - Results in `NUM_OF_STRATIFIED_CLIENTS` clients. Each client will have a stratified sample of the data.\n",
    "\n",
    "`MISSING_1_ATTACK` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 32 attack labels.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 6 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n",
    "\n",
    "`1_ATTACK_ONLY` with:\n",
    " - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 1 attack label.\n",
    " - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 1 attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n",
    "\n",
    "`HALF_BENIGN_ONLY` with:\n",
    " - `individual_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 33 malicious attack labels.\n",
    " - `group_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 7 malicious attack groups.\n",
    " - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and Malicious attack labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mSTRATIFIED METHOD\u001b[0m with 34 class classifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define fl_X_train and fl_y_train\n",
    "fl_X_train = []\n",
    "fl_y_train = []\n",
    "\n",
    "if METHOD == 'STRATIFIED':\n",
    "    print(f\"{Colours.YELLOW.value}STRATIFIED METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "    # We are going to split the training data into 'NUM_OF_STRATIFIED_CLIENTS' smaller groups using StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=NUM_OF_STRATIFIED_CLIENTS, shuffle=True, random_state=42)\n",
    "    for train_index, test_index in skf.split(train_df[X_columns], train_df[y_column]):\n",
    "        fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "        fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "\n",
    "elif METHOD == 'MISSING_1_ATTACK':\n",
    "    print(f\"{Colours.YELLOW.value}MISSING_1_ATTACK METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
    "\n",
    "    if individual_classifier or group_classifier:\n",
    "        # Set the number of splits required to the number of classes - 1\n",
    "        num_splits = int(class_size) - 1\n",
    "    else:\n",
    "        # For binary classifier, set the number of splits to 10\n",
    "        num_splits = 10\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # When creating the clients, we will remove one attack class from the training data\n",
    "    # For the binary classifier, evey other client will have the benign class removed\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
    "        if binary_classifier:\n",
    "            print(f\"i: {i} = i % 2 = {i % 2}\")\n",
    "            if i % 2 == 0:\n",
    "                print(\"Benign only\")\n",
    "                # Create a new dataframe for the client data with only benign traffic\n",
    "                client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != 1]], ignore_index=True)\n",
    "                fl_X_train.append(client_df[X_columns])\n",
    "                fl_y_train.append(client_df[y_column])\n",
    "            else:\n",
    "                print(\"Both\")\n",
    "                # Create a new dataframe for the client data\n",
    "                fl_X_train.append(train_df[X_columns].iloc[test_index])\n",
    "                fl_y_train.append(train_df[y_column].iloc[test_index])\n",
    "        else:\n",
    "            # Create a new dataframe for the client data\n",
    "            client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != i+1]], ignore_index=True)\n",
    "            fl_X_train.append(client_df[X_columns])\n",
    "            fl_y_train.append(client_df[y_column])\n",
    "\n",
    "elif METHOD == 'ATTACK_GROUP':\n",
    "    print(f\"{Colours.YELLOW.value}ATTACK_GROUP METHOD{Colours.NORMAL.value}\")\n",
    "    # With this method we split the data so that each client data sees all attacks except one. \n",
    "    # All clients will see attack traffic BenignTraffic - 0.\n",
    "    # EG:\n",
    "    # client 0 will see attacks 2-7\n",
    "    # client 1 will see attacks 1, 3-7\n",
    "    # client 2 will see attacks 1-2, 4-7\n",
    "    \n",
    "    # There are 7 attack groups + 1 benign class, so we will create 7 clients\n",
    "    skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
    "        # Create a new dataframe for the client data\n",
    "        client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != i+1]], ignore_index=True)\n",
    "        fl_X_train.append(client_df[X_columns])\n",
    "        fl_y_train.append(client_df[y_column])\n",
    "        \n",
    "    pass  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fl_X_train[0].shape: (142529, 46)\n",
      "fl_y_train[0].value_counts():\n",
      "6     21926\n",
      "4     16426\n",
      "5     13769\n",
      "2     12541\n",
      "3     12394\n",
      "1     12362\n",
      "7     11051\n",
      "13    10147\n",
      "15     8162\n",
      "14     6195\n",
      "0      3348\n",
      "17     3029\n",
      "19     2701\n",
      "18     2279\n",
      "10     1389\n",
      "26      926\n",
      "9       880\n",
      "8       868\n",
      "25      538\n",
      "24      411\n",
      "21      306\n",
      "22      258\n",
      "16      218\n",
      "23      116\n",
      "12       89\n",
      "11       73\n",
      "33       40\n",
      "27       22\n",
      "32       17\n",
      "31       16\n",
      "29       10\n",
      "28       10\n",
      "20        7\n",
      "30        5\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[0].unique(): [ 2  5  7  6  1  8  3 15  4 13 19 18 10  0 14 23 17 25 16  9 26 22 24 21\n",
      " 27 12 29 11 33 32 20 28 31 30]\n",
      "\n",
      "fl_X_train[1].shape: (142529, 46)\n",
      "fl_y_train[1].value_counts():\n",
      "6     21926\n",
      "4     16426\n",
      "5     13768\n",
      "2     12541\n",
      "3     12395\n",
      "1     12362\n",
      "7     11051\n",
      "13    10147\n",
      "15     8162\n",
      "14     6195\n",
      "0      3348\n",
      "17     3029\n",
      "19     2701\n",
      "18     2279\n",
      "10     1389\n",
      "26      926\n",
      "9       881\n",
      "8       868\n",
      "25      538\n",
      "24      410\n",
      "21      306\n",
      "22      258\n",
      "16      218\n",
      "23      116\n",
      "12       88\n",
      "11       73\n",
      "33       40\n",
      "27       22\n",
      "32       18\n",
      "31       16\n",
      "29       10\n",
      "28        9\n",
      "20        8\n",
      "30        5\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[1].unique(): [10 13 15 14  6  4  2  3  1  5  9  7 17 19  0 18 21  8 11 23 24 22 26 16\n",
      " 25 32 28 12 27 33 29 31 30 20]\n",
      "\n",
      "fl_X_train[2].shape: (142529, 46)\n",
      "fl_y_train[2].value_counts():\n",
      "6     21926\n",
      "4     16426\n",
      "5     13768\n",
      "2     12541\n",
      "3     12395\n",
      "1     12362\n",
      "7     11051\n",
      "13    10147\n",
      "15     8162\n",
      "14     6195\n",
      "0      3349\n",
      "17     3029\n",
      "19     2701\n",
      "18     2279\n",
      "10     1389\n",
      "26      925\n",
      "9       881\n",
      "8       868\n",
      "25      538\n",
      "24      410\n",
      "21      306\n",
      "22      258\n",
      "16      218\n",
      "23      116\n",
      "12       88\n",
      "11       73\n",
      "33       40\n",
      "27       22\n",
      "32       18\n",
      "31       16\n",
      "29       10\n",
      "28        9\n",
      "20        8\n",
      "30        5\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[2].unique(): [10  5  2 19  9  4  1  6  3 18 13 15  7  0 14 17 16  8 26 23 21 25 22 24\n",
      " 11 12 32 33 28 27 31 29 20 30]\n",
      "\n",
      "fl_X_train[3].shape: (142529, 46)\n",
      "fl_y_train[3].value_counts():\n",
      "6     21927\n",
      "4     16426\n",
      "5     13768\n",
      "2     12541\n",
      "3     12395\n",
      "1     12362\n",
      "7     11051\n",
      "13    10147\n",
      "15     8162\n",
      "14     6195\n",
      "0      3349\n",
      "17     3028\n",
      "19     2701\n",
      "18     2280\n",
      "10     1388\n",
      "26      925\n",
      "9       881\n",
      "8       868\n",
      "25      538\n",
      "24      410\n",
      "21      307\n",
      "22      258\n",
      "16      218\n",
      "23      116\n",
      "12       88\n",
      "11       72\n",
      "33       41\n",
      "27       21\n",
      "32       18\n",
      "31       16\n",
      "29       11\n",
      "28        9\n",
      "20        8\n",
      "30        4\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[3].unique(): [13  1  5  6  7 15 24 14  2  0  4 17  3 26 18 19  8 10  9 21 16 11 29 25\n",
      " 12 22 23 33 27 31 32 28 20 30]\n",
      "\n",
      "fl_X_train[4].shape: (142529, 46)\n",
      "fl_y_train[4].value_counts():\n",
      "6     21927\n",
      "4     16426\n",
      "5     13768\n",
      "2     12541\n",
      "3     12395\n",
      "1     12362\n",
      "7     11051\n",
      "13    10147\n",
      "15     8162\n",
      "14     6195\n",
      "0      3349\n",
      "17     3028\n",
      "19     2701\n",
      "18     2279\n",
      "10     1388\n",
      "26      925\n",
      "9       881\n",
      "8       868\n",
      "25      538\n",
      "24      410\n",
      "21      307\n",
      "22      259\n",
      "16      218\n",
      "23      116\n",
      "12       88\n",
      "11       72\n",
      "33       41\n",
      "27       21\n",
      "32       18\n",
      "31       16\n",
      "29       11\n",
      "28        9\n",
      "20        8\n",
      "30        4\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[4].unique(): [ 7  6 13  4  5 14 15  3  2  1 24  9 18 10 19  8 26  0 17 25 11 21 22 16\n",
      " 23 33 12 29 31 20 27 28 32 30]\n",
      "\n",
      "fl_X_train[5].shape: (142529, 46)\n",
      "fl_y_train[5].value_counts():\n",
      "6     21927\n",
      "4     16426\n",
      "5     13768\n",
      "2     12541\n",
      "3     12395\n",
      "1     12362\n",
      "7     11051\n",
      "13    10147\n",
      "15     8162\n",
      "14     6194\n",
      "0      3348\n",
      "17     3029\n",
      "19     2701\n",
      "18     2279\n",
      "10     1388\n",
      "26      925\n",
      "9       881\n",
      "8       868\n",
      "25      537\n",
      "24      411\n",
      "21      307\n",
      "22      259\n",
      "16      219\n",
      "23      116\n",
      "12       88\n",
      "11       72\n",
      "33       41\n",
      "27       21\n",
      "32       18\n",
      "31       16\n",
      "29       11\n",
      "28        9\n",
      "20        8\n",
      "30        4\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[5].unique(): [ 4 15  2  5  3  1  7 22  9  6 13 18 14 19 24  8 17  0 12 26 10 21 25 16\n",
      " 11 29 23 31 30 28 33 27 20 32]\n",
      "\n",
      "fl_X_train[6].shape: (142529, 46)\n",
      "fl_y_train[6].value_counts():\n",
      "6     21926\n",
      "4     16426\n",
      "5     13769\n",
      "2     12541\n",
      "3     12395\n",
      "1     12363\n",
      "7     11050\n",
      "13    10147\n",
      "15     8162\n",
      "14     6194\n",
      "0      3348\n",
      "17     3029\n",
      "19     2701\n",
      "18     2279\n",
      "10     1388\n",
      "26      925\n",
      "9       881\n",
      "8       868\n",
      "25      538\n",
      "24      411\n",
      "21      306\n",
      "22      258\n",
      "16      218\n",
      "23      117\n",
      "12       88\n",
      "11       73\n",
      "33       41\n",
      "27       21\n",
      "32       18\n",
      "31       16\n",
      "29       11\n",
      "28        9\n",
      "20        8\n",
      "30        4\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[6].unique(): [ 3  5  6  2 13  7 15 26  1 17  4 14 19 18 11  0 10 24 30 22 25  9  8 12\n",
      " 21 23 27 33 16 32 28 31 20 29]\n",
      "\n",
      "fl_X_train[7].shape: (142528, 46)\n",
      "fl_y_train[7].value_counts():\n",
      "6     21926\n",
      "4     16425\n",
      "5     13769\n",
      "2     12541\n",
      "3     12395\n",
      "1     12363\n",
      "7     11050\n",
      "13    10147\n",
      "15     8162\n",
      "14     6194\n",
      "0      3348\n",
      "17     3029\n",
      "19     2701\n",
      "18     2279\n",
      "10     1389\n",
      "26      926\n",
      "9       881\n",
      "8       867\n",
      "25      538\n",
      "24      411\n",
      "21      306\n",
      "22      258\n",
      "16      218\n",
      "23      117\n",
      "12       88\n",
      "11       73\n",
      "33       41\n",
      "27       21\n",
      "32       18\n",
      "31       16\n",
      "29       11\n",
      "28        9\n",
      "20        7\n",
      "30        4\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[7].unique(): [19  5  7 15  2  6 13 18  4 26 25 14  3  1 17 21  0 10  9 22 24 11  8 31\n",
      " 16 33 12 27 23 20 32 30 28 29]\n",
      "\n",
      "fl_X_train[8].shape: (142528, 46)\n",
      "fl_y_train[8].value_counts():\n",
      "6     21926\n",
      "4     16425\n",
      "5     13769\n",
      "2     12541\n",
      "3     12395\n",
      "1     12363\n",
      "7     11050\n",
      "13    10147\n",
      "15     8162\n",
      "14     6194\n",
      "0      3348\n",
      "17     3029\n",
      "19     2701\n",
      "18     2279\n",
      "10     1389\n",
      "26      926\n",
      "9       881\n",
      "8       867\n",
      "25      538\n",
      "24      411\n",
      "21      306\n",
      "22      258\n",
      "16      218\n",
      "23      116\n",
      "12       89\n",
      "11       73\n",
      "33       41\n",
      "27       22\n",
      "32       18\n",
      "31       15\n",
      "29       11\n",
      "28        9\n",
      "20        7\n",
      "30        4\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[8].unique(): [ 4 13  1  3 15  0  7  5  6  2 19 25  9 14 18 10 11 24 17 32 21  8 16 26\n",
      " 12 33 29 22 27 23 20 30 31 28]\n",
      "\n",
      "fl_X_train[9].shape: (142528, 46)\n",
      "fl_y_train[9].value_counts():\n",
      "6     21926\n",
      "4     16426\n",
      "5     13769\n",
      "2     12541\n",
      "3     12394\n",
      "1     12362\n",
      "7     11051\n",
      "13    10146\n",
      "15     8162\n",
      "14     6195\n",
      "0      3348\n",
      "17     3029\n",
      "19     2701\n",
      "18     2279\n",
      "10     1389\n",
      "26      926\n",
      "9       881\n",
      "8       867\n",
      "25      538\n",
      "24      411\n",
      "21      306\n",
      "22      258\n",
      "16      218\n",
      "23      116\n",
      "12       89\n",
      "11       73\n",
      "33       40\n",
      "27       22\n",
      "32       18\n",
      "31       16\n",
      "28       10\n",
      "29       10\n",
      "20        7\n",
      "30        4\n",
      "Name: label, dtype: int64\n",
      "fl_y_train[9].unique(): [ 2  6  5  1 13 19  7 15  0 18 14 17  4  3 10 25 24  9  8 26 16 33 21 22\n",
      " 12 23 28 11 27 20 31 30 29 32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_CLIENTS = len(fl_X_train)\n",
    "\n",
    "for i in range(len(fl_X_train)):\n",
    "    # Show the unique values in the y column\n",
    "    (f\"Client ID: {i}\")\n",
    "    print(f\"fl_X_train[{i}].shape: {fl_X_train[i].shape}\")  \n",
    "    print(f\"fl_y_train[{i}].value_counts():\\n{fl_y_train[i].value_counts()}\")\n",
    "    print(f\"fl_y_train[{i}].unique(): {fl_y_train[i].unique()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare an output directory where we can store the results of the federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an \"Output\" directory if it doesnt exist already\n",
    "if not os.path.exists(\"Output\"):\n",
    "    os.makedirs(\"Output\")\n",
    "\n",
    "sub_directory_name = f\"Output/{METHOD}_Classifier-{class_size}_Clients-{NUM_OF_CLIENTS}_Rounds-{NUM_OF_ROUNDS}\"\n",
    "\n",
    "# Create an \"Output/{METHOD}-{NUM_OF_CLIENTS}-{NUM_OF_ROUNDS}\" directory if it doesnt exist already\n",
    "if not os.path.exists(f\"Output/{sub_directory_name}\"):\n",
    "    os.makedirs(f\"Output/{sub_directory_name}\")\n",
    "\n",
    "# Ensure the directory is empty\n",
    "for file in os.listdir(f\"Output/{sub_directory_name}\"):\n",
    "    file_path = os.path.join(f\"Output/{sub_directory_name}\", file)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.unlink(file_path)\n",
    "\n",
    "# Write this same info to the output directory/Class Split Info.txt\n",
    "with open(f\"Output/{sub_directory_name}/Class Split Info.txt\", \"w\") as f:\n",
    "    for i in range(len(fl_X_train)):\n",
    "        f.write(f\"Client ID: {i}\\n\")\n",
    "        f.write(f\"fl_X_train.shape: {fl_X_train[0].shape}\\n\")\n",
    "        f.write(f\"fl_y_train.value_counts():\\n{fl_y_train[0].value_counts()}\\n\")\n",
    "        f.write(f\"fl_y_train.unique(): {fl_y_train[0].unique()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the testing daya to X_test and y_test ndarrays\n",
    "X_test = test_df[X_columns].to_numpy()\n",
    "y_test = test_df[y_column].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CLIENTS: 10\n",
      "NUM_ROUNDS: 10\n",
      "\n",
      "Original train_df size: (1425287, 47)\n",
      "Checking training data split groups\n",
      "0 : X Shape (142529, 46) Y Shape (142529,)\n",
      "1 : X Shape (142529, 46) Y Shape (142529,)\n",
      "2 : X Shape (142529, 46) Y Shape (142529,)\n",
      "3 : X Shape (142529, 46) Y Shape (142529,)\n",
      "4 : X Shape (142529, 46) Y Shape (142529,)\n",
      "5 : X Shape (142529, 46) Y Shape (142529,)\n",
      "6 : X Shape (142529, 46) Y Shape (142529,)\n",
      "7 : X Shape (142528, 46) Y Shape (142528,)\n",
      "8 : X Shape (142528, 46) Y Shape (142528,)\n",
      "9 : X Shape (142528, 46) Y Shape (142528,)\n",
      "\n",
      "Checking testing data\n",
      "X_test size: (462433, 46)\n",
      "y_test size: (462433,)\n",
      "\n",
      "Deploy Simulation\n"
     ]
    }
   ],
   "source": [
    "print(\"NUM_CLIENTS:\", NUM_OF_CLIENTS)\n",
    "\n",
    "print(\"NUM_ROUNDS:\", NUM_OF_ROUNDS)\n",
    "print()\n",
    "print(\"Original train_df size: {}\".format(train_df.shape))\n",
    "\n",
    "print(\"Checking training data split groups\")\n",
    "for i in range(len(fl_X_train)):\n",
    "    print(i, \":\", \"X Shape\", fl_X_train[i].shape, \"Y Shape\", fl_y_train[i].shape)\n",
    "\n",
    "\n",
    "# Print the sizes of X_test and y_test\n",
    "print(\"\\nChecking testing data\")\n",
    "print(\"X_test size: {}\".format(X_test.shape))\n",
    "print(\"y_test size: {}\".format(y_test.shape))\n",
    "\n",
    "print(\"\\nDeploy Simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Federated Learning\n",
    "## Import the libraries and print the versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "label = train_df[y_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Client and Server code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn 1.2.0.\n",
      "flwr 1.4.0\n",
      "numpy 1.24.2\n",
      "tf 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print('scikit-learn {}.'.format(sklearn.__version__))\n",
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"tf\", tf.__version__)\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "import datetime\n",
    "\n",
    "client_evaluations = []\n",
    "\n",
    "class NumpyFlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, model, train_data, train_labels):\n",
    "        self.model = model\n",
    "        self.cid = cid\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Training...\")\n",
    "        self.model.fit(self.train_data, self.train_labels, epochs=5, batch_size=32)\n",
    "        print (\"Client \", self.cid, \"Training complete...\")\n",
    "        return self.model.get_weights(), len(self.train_data), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Evaluating...\")\n",
    "        loss, accuracy = self.model.evaluate(self.train_data, self.train_labels, batch_size=32)\n",
    "        print(f\"{Colours.YELLOW.value}Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}{Colours.NORMAL.value}\")\n",
    "\n",
    "        # Write the same message to the \"Output/{cid}_Evaluation.txt\" file\n",
    "        with open(f\"Output/{sub_directory_name}/{self.cid}_Evaluation.txt\", \"a\") as f:\n",
    "            f.write(f\"{datetime.datetime.now()} - Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}\\n\")\n",
    "\n",
    "            # Close the file\n",
    "            f.close()\n",
    "\n",
    "        return loss, len(self.train_data), {\"accuracy\": accuracy}\n",
    "    \n",
    "    def predict(self, incoming):\n",
    "        prediction = np.argmax( self.model.predict(incoming) ,axis=1)\n",
    "        return prediction\n",
    "\n",
    "def client_fn(cid: str) -> NumpyFlowerClient:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    #model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n",
    "    #model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    print (\"Client ID:\", cid)\n",
    "\n",
    "    model = Sequential([\n",
    "      #Flatten(input_shape=(79,1)),\n",
    "      Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "      Dense(50, activation='relu'),  \n",
    "      Dense(25, activation='relu'),  \n",
    "      Dense(len(label.unique()), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "   \n",
    "    partition_id = int(cid)\n",
    "    X_train_c = fl_X_train[partition_id]\n",
    "    y_train_c = fl_y_train[partition_id]\n",
    "\n",
    "    # Create a  single Flower client representing a single organization\n",
    "    return NumpyFlowerClient(cid, model, X_train_c, y_train_c)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "eval_count = 0\n",
    "\n",
    "def get_evaluate_fn(server_model):\n",
    "    global eval_count\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "    # The `evaluate` function will be called after every round\n",
    "    \n",
    "    \n",
    "    def evaluate(server_round, parameters, config):\n",
    "        global eval_count\n",
    "        \n",
    "        # Update model with the latest parameters\n",
    "        server_model.set_weights(parameters)\n",
    "        print (f\"Server Evaluating... Evaluation Count:{eval_count}\")\n",
    "        loss, accuracy = server_model.evaluate(X_test, y_test)\n",
    "        \n",
    "        y_pred = server_model.predict(X_test)\n",
    "        print (\"Prediction: \", y_pred, y_pred.shape)\n",
    "        #cmatrix = confusion_matrix(y_test, np.rint(y_pred))\n",
    "        #print (\"confusion_matrix:\", cmatrix, cmatrix.shape)\n",
    "                        \n",
    "        print(f\"{Colours.YELLOW.value}Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}{Colours.NORMAL.value}\")\n",
    "\n",
    "        # Write the same message to the \"Output/Server_Evaluation.txt\" file\n",
    "        with open(f\"Output/{sub_directory_name}/Server_Evaluation.txt\", \"a\") as f:\n",
    "            f.write(f\"{datetime.datetime.now()} - {server_round} : Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\\n\")\n",
    "\n",
    "            # Close the file\n",
    "            f.close()\n",
    "        \n",
    "        np.save(\"y_pred-\" + str(eval_count) + \".npy\", y_pred)\n",
    "        #np.save(\"cmatrix-\" + str(eval_count) + \".npy\", cmatrix)\n",
    "        eval_count = eval_count + 1\n",
    "        \n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "    return evaluate\n",
    "\n",
    "\n",
    "\n",
    "server_model = Sequential([\n",
    "    #Flatten(input_shape=(79,1)),\n",
    "    Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "    Dense(50, activation='relu'),  \n",
    "    Dense(25, activation='relu'),  \n",
    "    Dense(len(label.unique()), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "server_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=2, #10,\n",
    "        min_evaluate_clients=2, #5,\n",
    "        min_available_clients=2, #10,\n",
    "        evaluate_fn=get_evaluate_fn(server_model),\n",
    "        #evaluate_metrics_aggregation_fn=weighted_average,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-07-09 19:17:30,633 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\n",
      "Deploy simulation... Method = STRATIFIED - Individual (34) Classifier\n",
      "Number of Clients = 10\u001b[0m\n",
      "\n",
      "Writing output to: Output/STRATIFIED_Classifier-34_Clients-10_Rounds-10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 19:17:36,762\tINFO worker.py:1636 -- Started a local Ray instance.\n",
      "INFO flwr 2023-07-09 19:17:41,890 | app.py:180 | Flower VCE: Ray initialized with resources: {'CPU': 24.0, 'GPU': 1.0, 'node:127.0.0.1': 1.0, 'memory': 33624657102.0, 'object_store_memory': 16812328550.0}\n",
      "INFO flwr 2023-07-09 19:17:41,891 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-07-09 19:17:41,892 | server.py:273 | Requesting initial parameters from one random client\n",
      "INFO flwr 2023-07-09 19:17:46,764 | server.py:277 | Received initial parameters from one random client\n",
      "INFO flwr 2023-07-09 19:17:46,765 | server.py:88 | Evaluating initial parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=24420)\u001b[0m Client ID: 9\n",
      "Server Evaluating... Evaluation Count:0\n",
      " 2893/14452 [=====>........................] - ETA: 9s - loss: 3.5888 - accuracy: 0.0364"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:27\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\flwr\\simulation\\app.py:197\u001b[0m, in \u001b[0;36mstart_simulation\u001b[1;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised)\u001b[0m\n\u001b[0;32m    194\u001b[0m     initialized_server\u001b[39m.\u001b[39mclient_manager()\u001b[39m.\u001b[39mregister(client\u001b[39m=\u001b[39mclient_proxy)\n\u001b[0;32m    196\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m hist \u001b[39m=\u001b[39m _fl(\n\u001b[0;32m    198\u001b[0m     server\u001b[39m=\u001b[39;49minitialized_server,\n\u001b[0;32m    199\u001b[0m     config\u001b[39m=\u001b[39;49minitialized_config,\n\u001b[0;32m    200\u001b[0m )\n\u001b[0;32m    202\u001b[0m event(EventType\u001b[39m.\u001b[39mSTART_SIMULATION_LEAVE)\n\u001b[0;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m hist\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\flwr\\server\\app.py:217\u001b[0m, in \u001b[0;36m_fl\u001b[1;34m(server, config)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fl\u001b[39m(\n\u001b[0;32m    213\u001b[0m     server: Server,\n\u001b[0;32m    214\u001b[0m     config: ServerConfig,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m History:\n\u001b[0;32m    216\u001b[0m     \u001b[39m# Fit model\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m     hist \u001b[39m=\u001b[39m server\u001b[39m.\u001b[39;49mfit(num_rounds\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rounds, timeout\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mround_timeout)\n\u001b[0;32m    218\u001b[0m     log(INFO, \u001b[39m\"\u001b[39m\u001b[39mapp_fit: losses_distributed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(hist\u001b[39m.\u001b[39mlosses_distributed))\n\u001b[0;32m    219\u001b[0m     log(INFO, \u001b[39m\"\u001b[39m\u001b[39mapp_fit: metrics_distributed_fit \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(hist\u001b[39m.\u001b[39mmetrics_distributed_fit))\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\flwr\\server\\server.py:89\u001b[0m, in \u001b[0;36mServer.fit\u001b[1;34m(self, num_rounds, timeout)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_initial_parameters(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m     88\u001b[0m log(INFO, \u001b[39m\"\u001b[39m\u001b[39mEvaluating initial parameters\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mevaluate(\u001b[39m0\u001b[39;49m, parameters\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters)\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     log(\n\u001b[0;32m     92\u001b[0m         INFO,\n\u001b[0;32m     93\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minitial parameters (loss, other metrics): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m         res[\u001b[39m0\u001b[39m],\n\u001b[0;32m     95\u001b[0m         res[\u001b[39m1\u001b[39m],\n\u001b[0;32m     96\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\flwr\\server\\strategy\\fedavg.py:164\u001b[0m, in \u001b[0;36mFedAvg.evaluate\u001b[1;34m(self, server_round, parameters)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    163\u001b[0m parameters_ndarrays \u001b[39m=\u001b[39m parameters_to_ndarrays(parameters)\n\u001b[1;32m--> 164\u001b[0m eval_res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_fn(server_round, parameters_ndarrays, {})\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m eval_res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Jon\\Documents\\VSCode Projects\\CICIoT2023\\03-Federated Learning.ipynb Cell 41\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jon/Documents/VSCode%20Projects/CICIoT2023/03-Federated%20Learning.ipynb#Y230sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m server_model\u001b[39m.\u001b[39mset_weights(parameters)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jon/Documents/VSCode%20Projects/CICIoT2023/03-Federated%20Learning.ipynb#Y230sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mServer Evaluating... Evaluation Count:\u001b[39m\u001b[39m{\u001b[39;00meval_count\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Jon/Documents/VSCode%20Projects/CICIoT2023/03-Federated%20Learning.ipynb#Y230sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m loss, accuracy \u001b[39m=\u001b[39m server_model\u001b[39m.\u001b[39;49mevaluate(X_test, y_test)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jon/Documents/VSCode%20Projects/CICIoT2023/03-Federated%20Learning.ipynb#Y230sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m y_pred \u001b[39m=\u001b[39m server_model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jon/Documents/VSCode%20Projects/CICIoT2023/03-Federated%20Learning.ipynb#Y230sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mPrediction: \u001b[39m\u001b[39m\"\u001b[39m, y_pred, y_pred\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\keras\\engine\\training.py:2040\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2036\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   2037\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   2038\u001b[0m ):\n\u001b[0;32m   2039\u001b[0m     callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2040\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[0;32m   2041\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2042\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Jon\\anaconda3\\envs\\py310copy\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print (f\"{Colours.YELLOW.value}\\nDeploy simulation... Method = {METHOD} - {class_size_map[len(label.unique())]} ({class_size}) Classifier\")\n",
    "print (f\"Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
    "print (f\"Writing output to: {sub_directory_name}\\n{Colours.NORMAL.value}\")\n",
    "\n",
    "# Output the same information to the Output/Run_details.txt file\n",
    "with open(f\"Output/{sub_directory_name}/Run_details.txt\", \"a\") as f:\n",
    "    f.write(f\"{datetime.datetime.now()} - Deploy simulation... Method = {METHOD} - {class_size_map[len(label.unique())]} ({class_size}) Classifier\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
    "\n",
    "    # Write Original train_df size\n",
    "    f.write(f\"{datetime.datetime.now()} - Original train_df size: {train_df.shape}\\n\")\n",
    "\n",
    "    # Write the training data split groups\n",
    "    for i in range(len(fl_X_train)):\n",
    "        f.write(f\"{datetime.datetime.now()} - {i}: X Shape {fl_X_train[i].shape}, Y Shape {fl_y_train[i].shape}\\n\")\n",
    "\n",
    "    # Write the testing data\n",
    "    f.write(f\"{datetime.datetime.now()} - X_test size: {X_test.shape}\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - y_test size: {y_test.shape}\\n\")\n",
    "    \n",
    "# close the file\n",
    "f.close()\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_OF_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_OF_ROUNDS),\n",
    "    strategy=strategy,\n",
    ")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Total time taken: \", end_time - start_time)\n",
    "\n",
    "print (f\"{Colours.YELLOW.value} SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[len(label.unique())]} ({class_size}) Classifier\")\n",
    "print (f\"Number of Clients = {NUM_OF_CLIENTS}{Colours.NORMAL.value}\\n\")\n",
    "\n",
    "# Output the same information to the Output/Run_details.txt file\n",
    "with open(f\"Output/{sub_directory_name}/Run_details.txt\", \"a\") as f:\n",
    "    f.write(f\"{datetime.datetime.now()} - SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[len(label.unique())]} ({class_size}) Classifier\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()} - Total time taken: {end_time - start_time}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
